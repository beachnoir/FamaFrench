{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50e111d",
   "metadata": {},
   "source": [
    "# The Fama-French 3-Factor Model: A Complete Worked Example\n",
    "\n",
    "## A Step-by-Step Guide for Understanding Asset Pricing Through Multiple Linear Regression\n",
    "\n",
    "This notebook provides a comprehensive tutorial on the Fama-French 3-factor model—one of the most important tools in modern finance for understanding investment returns. We'll cover:\n",
    "\n",
    "1. **The Theory**: What is the Fama-French model and why is it important?\n",
    "2. **The Mathematics**: All formulas explained in detail\n",
    "3. **Real Data**: Using actual historical market data\n",
    "4. **Implementation**: Python code to perform the analysis\n",
    "5. **Interpretation**: What the results tell us about investments\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll understand:\n",
    "- How to conduct a multiple linear regression analysis\n",
    "- What the Fama-French 3 factors represent\n",
    "- How to interpret regression coefficients and test statistics\n",
    "- How to validate regression assumptions\n",
    "- How to evaluate model performance\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This material is designed for an undergraduate student with:\n",
    "- **Statistics**: Basic knowledge of mean, variance, hypothesis testing, and linear regression concepts\n",
    "- **Python**: Familiarity with Python syntax, NumPy arrays, and pandas DataFrames. You don't need to be an expert, but you should be comfortable reading Python code and understanding basic operations like indexing and function calls.\n",
    "- **Linear Algebra**: A basic understanding of matrix multiplication and transposes is helpful for Section 5 (the normal equations), but not strictly required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab101022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "# Note: We suppress some non-critical deprecation warnings from libraries.\n",
    "# In your own work, you may want to keep warnings visible to catch potential issues.\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Set visualization style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf9fe0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Understanding the Fama-French 3-Factor Model\n",
    "\n",
    "### The Theory Behind Factor Models\n",
    "\n",
    "The **Fama-French 3-Factor Model** is a framework developed by economists Eugene Fama and Kenneth French to explain stock returns. It extends the simpler Capital Asset Pricing Model (CAPM) by including two additional factors beyond just the market risk premium.\n",
    "\n",
    "The model answers this question: **Why do some stocks or portfolios outperform others?**\n",
    "\n",
    "### The Mathematical Model\n",
    "\n",
    "$$R_i(t) - R_f(t) = \\alpha_i + \\beta_{MKT} \\left[R_m(t) - R_f(t)\\right] + \\beta_{SMB} \\cdot SMB(t) + \\beta_{HML} \\cdot HML(t) + \\epsilon_i(t)$$\n",
    "\n",
    "**Where:**\n",
    "- $R_i(t)$ = Return of the asset (stock, ETF, or portfolio) at time $t$\n",
    "- $R_f(t)$ = Risk-free rate (typically the 1-month U.S. Treasury bill rate)\n",
    "- $R_m(t)$ = Return of the overall market portfolio\n",
    "- $\\alpha$ = **Alpha** (Jensen's alpha): The abnormal return not explained by the three factors. This represents the \"excess\" return the asset earns above what the model predicts. A positive alpha suggests outperformance.\n",
    "- $\\beta_{MKT}$ = **Market Beta**: Sensitivity to market risk. A beta of 1 means the asset moves with the market; >1 means more volatile; <1 means less volatile.\n",
    "- $\\beta_{SMB}$ = **SMB Beta**: Sensitivity to the small-minus-big factor, which captures the **size effect** (small firms tend to outperform large firms).\n",
    "- $\\beta_{HML}$ = **HML Beta**: Sensitivity to the high-minus-low factor, which captures the **value effect** (value stocks tend to outperform growth stocks).\n",
    "- $SMB(t)$ = **Small Minus Big factor**: The return difference between portfolios of small-cap and large-cap stocks\n",
    "- $HML(t)$ = **High Minus Low factor**: The return difference between portfolios of high book-to-market (value) and low book-to-market (growth) stocks\n",
    "- $\\epsilon_i(t)$ = Error term (random, unexplained variation)\n",
    "\n",
    "### Why Three Factors?\n",
    "\n",
    "Fama and French found that three characteristics systematically explain returns:\n",
    "\n",
    "1. **Market Risk Premium** $(R_m - R_f)$: Stocks are riskier than risk-free assets, so investors demand a premium for taking market risk.\n",
    "\n",
    "2. **Size Effect (SMB)**: Historically, small-cap stocks have outperformed large-cap stocks on average. Why? They're riskier, less liquid, and have less diversified business models.\n",
    "\n",
    "3. **Value Effect (HML)**: Stocks with high book-to-market ratios (cheap stocks relative to their book value) tend to outperform stocks with low book-to-market ratios (expensive growth stocks).\n",
    "\n",
    "### What We'll Do (Step-by-Step)\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "1. Obtain historical data on ETF returns and the three factors\n",
    "2. Run a regression to estimate $\\alpha$, $\\beta_{MKT}$, $\\beta_{SMB}$, and $\\beta_{HML}$\n",
    "3. Test the statistical significance of these coefficients\n",
    "4. Interpret what they tell us about the ETF's factor exposures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52aae19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Load and Explore the Data\n",
    "\n",
    "We'll use **real, actual market data** from two sources:\n",
    "\n",
    "1. **Fama-French Factors**: Downloaded from Kenneth French's official data library at https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n",
    "   - Market risk premium (Mkt-RF)\n",
    "   - Size effect factor (SMB)\n",
    "   - Value effect factor (HML)\n",
    "   - Risk-free rate (RF)\n",
    "\n",
    "2. **ETF Returns**: Downloaded from Yahoo Finance using real historical prices for three US-listed ETFs with very different expected factor exposures:\n",
    "   - **SPY** (SPDR S&P 500 ETF Trust) — a broad US market ETF tracking the S&P 500. We expect $\\beta_{MKT} \\approx 1$ and low SMB/HML loadings.\n",
    "   - **IWN** (iShares Russell 2000 Value ETF) — targets US small-cap value stocks. We expect **positive** SMB and HML loadings.\n",
    "   - **VTV** (Vanguard Value ETF) — targets US large-cap value stocks. We expect a **positive** HML loading but near-zero or negative SMB loading.\n",
    "\n",
    "### Why These Three ETFs?\n",
    "\n",
    "These ETFs are chosen deliberately because they let us *see the Fama-French factors at work*:\n",
    "- A broad US market ETF should be mostly explained by the market factor alone, with $\\beta_{MKT} \\approx 1$.\n",
    "- A US small-cap value ETF should load positively on **both** SMB and HML, confirming that it captures the size and value premiums.\n",
    "- A US large-cap value ETF lets us isolate the **value effect**—it should load on HML but not on SMB.\n",
    "\n",
    "**Why US-listed ETFs?** The Fama-French factors are constructed from US stocks (the CRSP universe). Using US-listed ETFs denominated in USD ensures a clean match between the factors and the assets, giving us the most interpretable regression results. If you wanted to analyze non-US assets, you would use Kenneth French's regional factor sets (e.g., European or Global factors) for a proper match.\n",
    "\n",
    "### How Are the Fama-French Factors Constructed?\n",
    "\n",
    "This is an important question for understanding where empirical factor data comes from. The construction is actually quite clever:\n",
    "\n",
    "**Market Risk Premium (Mkt-RF)**\n",
    "- Construct a value-weighted portfolio of all stocks in the market\n",
    "- Calculate its monthly return\n",
    "- Subtract the risk-free rate (typically 1-month Treasury bill yield)\n",
    "- Result: The excess return investors earn for taking market risk\n",
    "\n",
    "**SMB (Small Minus Big) - The Size Factor**\n",
    "1. Divide all stocks into two groups based on market capitalization:\n",
    "   - Small cap: Bottom 30% by market cap\n",
    "   - Large cap: Top 30% by market cap\n",
    "2. Construct two value-weighted portfolios:\n",
    "   - Portfolio S: Value-weighted return of all small-cap stocks\n",
    "   - Portfolio B: Value-weighted return of all large-cap stocks\n",
    "3. **SMB = Return(S) - Return(B)**\n",
    "   - Positive SMB means small stocks outperformed large stocks that month\n",
    "   - Negative SMB means large stocks outperformed small stocks\n",
    "\n",
    "**Why \"size\" is measured as log(market cap), not raw market cap.** In the academic literature (and in Fama-French's own construction), firm size means $\\log(\\text{market cap})$. Why the logarithm?\n",
    "\n",
    "1. **The distribution of raw market caps is extremely right-skewed.** Apple might be worth $3 trillion while a micro-cap firm is worth $50 million — a 60,000× difference. Taking logs compresses this range: $\\log(3 \\times 10^{12}) \\approx 28.7$ vs. $\\log(5 \\times 10^{7}) \\approx 17.7$ — now a manageable 1.6× ratio.\n",
    "2. **Multiplicative effects become additive.** A firm that doubles in market cap shifts by $\\log 2 \\approx 0.69$ regardless of whether it went from \\\\$100M to \\\\$200M or from \\\\$10B to \\\\$20B. This matches the economic intuition that *proportional* changes in size matter, not absolute dollar changes.\n",
    "3. **Regression behaves better.** OLS assumes errors have roughly constant variance. Regressing returns on raw market cap would give extreme leverage to the handful of mega-cap stocks, violating that assumption. Log market cap produces a much more uniform spread.\n",
    "4. **Historical convention.** Banz (1981), the paper that first documented the size effect, used log(market cap). Fama and French followed suit, and the convention stuck.\n",
    "\n",
    "In the Fama-French factor construction, stocks are sorted into small vs. big groups using the **median NYSE market cap** as the breakpoint (not a log-scale cutoff), but whenever size appears as a continuous variable in a regression (e.g., cross-sectional regressions like Fama-MacBeth), it is always logged.\n",
    "\n",
    "**HML (High Minus Low) - The Value Factor**\n",
    "1. Divide all stocks into two groups based on book-to-market ratio (P/B inverse):\n",
    "   - High B/M (Value stocks): Top 30% by book-to-market ratio (cheap stocks)\n",
    "   - Low B/M (Growth stocks): Bottom 30% by book-to-market ratio (expensive stocks)\n",
    "2. Construct two value-weighted portfolios:\n",
    "   - Portfolio H: Value-weighted return of all high book-to-market stocks\n",
    "   - Portfolio L: Value-weighted return of all low book-to-market stocks\n",
    "3. **HML = Return(H) - Return(L)**\n",
    "   - Positive HML means value stocks outperformed growth stocks that month\n",
    "   - Negative HML means growth stocks outperformed value stocks\n",
    "\n",
    "**Key Insights:**\n",
    "- These factors are **long-short portfolios**: They go long (buy) one group and short (sell) another\n",
    "- They capture systematic **risk premiums** that aren't explained by pure market risk\n",
    "- The factors are **zero-net-cost**: You can replicate them by borrowing to fund positions\n",
    "- They're constructed from **all available US stocks**, making them broad market-based factors\n",
    "- Kenneth French updates these factors monthly, making them ideal for empirical research\n",
    "\n",
    "**Data Quality:**\n",
    "- Factors have been calculated since 1926\n",
    "- We use **2010–2023** data in this tutorial (giving us a substantial sample of over 160 months). This shorter window keeps the tutorial focused on a relatively stable market regime; the [Time-Series Foundations](02_Time_Series_Foundations.ipynb) and [Advanced Factor Models](04_Advanced_Factor_Models.ipynb) notebooks use the longer **2000–2023** window to include the dot-com bust and global financial crisis, which add power for studying time-varying factor dynamics.\n",
    "- Values are expressed as **percentages** (e.g., 1.23 means +1.23% return that month)\n",
    "- The data quality is excellent because it comes directly from the CRSP database (maintained by the University of Chicago)\n",
    "\n",
    "Our 2010–2023 sample provides approximately 168 months of data, which is a solid foundation for this analysis.\n",
    "\n",
    "### A Note on Sample Size\n",
    "\n",
    "Professional empirical research typically uses 10+ years of monthly data (120+ observations) to obtain reliable estimates. A larger sample:\n",
    "\n",
    "- **Increases statistical power** (more likely to detect true effects)\n",
    "- **Reduces standard errors** (more data = less uncertainty)\n",
    "- **Makes diagnostic tests more reliable** (e.g., the Jarque-Bera test has low power with fewer than ~50 observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ff49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Download Fama-French 3-Factor Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Downloading Fama-French factor data from Kenneth French's data library...\")\n",
    "\n",
    "import yfinance as yf\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\"\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    zip_path = f\"{tmpdir}/ff_data.zip\"\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(tmpdir)\n",
    "    \n",
    "    files = os.listdir(tmpdir)\n",
    "    csv_file = None\n",
    "    for f in files:\n",
    "        if f.lower().endswith('.csv') and 'Factor' in f:\n",
    "            csv_file = f\"{tmpdir}/{f}\"\n",
    "            break\n",
    "    \n",
    "    if not csv_file:\n",
    "        raise ValueError(f\"No CSV file found. Files: {files}\")\n",
    "    \n",
    "    # --- Robust CSV parsing ---\n",
    "    # Read the entire CSV, then filter to valid monthly rows programmatically\n",
    "    # (instead of guessing where the \"Annual\" section starts).\n",
    "    ff_factors = pd.read_csv(csv_file, skiprows=4, index_col=0)\n",
    "\n",
    "# Clean up the index and keep only rows whose index looks like a 6-digit YYYYMM date\n",
    "ff_factors.index = pd.Index(ff_factors.index.astype(str).str.strip())\n",
    "ff_factors = ff_factors[ff_factors.index.str.match(r'^\\d{6}$')]\n",
    "\n",
    "# Convert index to datetime\n",
    "ff_factors.index = pd.to_datetime(ff_factors.index, format='%Y%m')\n",
    "\n",
    "# Drop any rows with missing values\n",
    "ff_factors = ff_factors.dropna()\n",
    "\n",
    "# Ensure columns are numeric\n",
    "ff_factors = ff_factors.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "# Rename columns\n",
    "ff_factors.columns = ['Mkt_RF', 'SMB', 'HML', 'RF']\n",
    "\n",
    "# Filter to 2010-2023 period (long sample for reliable estimates)\n",
    "ff_factors = ff_factors.loc['2010-01-01':'2023-12-31']\n",
    "\n",
    "# Convert from percentages to decimals\n",
    "ff_factors = ff_factors / 100\n",
    "\n",
    "print(f\"Done! Shape: {ff_factors.shape}\")\n",
    "print(f\"Date range: {ff_factors.index[0].date()} to {ff_factors.index[-1].date()}\")\n",
    "print(f\"Number of months: {len(ff_factors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2632fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Download ETF Price Data from Yahoo Finance\n",
    "# ============================================================================\n",
    "# We use three US-listed ETFs with very different factor exposures:\n",
    "#   SPY  = SPDR S&P 500 ETF Trust (broad US market)\n",
    "#   IWN  = iShares Russell 2000 Value ETF (US small-cap value)\n",
    "#   VTV  = Vanguard Value ETF (US large-cap value)\n",
    "# ============================================================================\n",
    "\n",
    "etf_tickers = {\n",
    "    'SPY':  'SPY (S&P 500)',\n",
    "    'IWN':  'IWN (Small Cap Value)',\n",
    "    'VTV':  'VTV (Large Cap Value)',\n",
    "}\n",
    "\n",
    "print(\"Downloading ETF price data from Yahoo Finance...\\n\")\n",
    "\n",
    "etf_dataframes = {}\n",
    "\n",
    "for ticker, label in etf_tickers.items():\n",
    "    print(f\"  Downloading {label}...\")\n",
    "    \n",
    "    raw = yf.download(ticker, start='2010-01-01', end='2023-12-31', progress=False)\n",
    "    \n",
    "    # Get Adjusted Close Price — handle different yfinance return types\n",
    "    if isinstance(raw, pd.Series):\n",
    "        close_price = raw\n",
    "    else:\n",
    "        if isinstance(raw.columns, pd.MultiIndex):\n",
    "            try:\n",
    "                close_price = raw[('Adj Close', ticker)]\n",
    "            except KeyError:\n",
    "                close_price = raw.iloc[:, raw.columns.get_level_values(1) == ticker]\n",
    "                close_price = close_price.iloc[:, 0]\n",
    "        else:\n",
    "            if 'Adj Close' in raw.columns:\n",
    "                close_price = raw['Adj Close']\n",
    "            else:\n",
    "                close_price = raw.iloc[:, 0]\n",
    "    \n",
    "    # Calculate monthly returns (resample to start-of-month)\n",
    "    monthly = close_price.resample('MS').first()\n",
    "    returns = monthly.pct_change().dropna()\n",
    "    \n",
    "    # Align dates with Fama-French calendar.\n",
    "    # Note: pct_change() at Feb 1 gives the Jan→Feb return, but the\n",
    "    # Fama-French data labels January's return at the Jan 1 date.\n",
    "    # We handle this by matching available return dates to the FF index\n",
    "    # rather than shifting, since both sources use start-of-month dates\n",
    "    # in this download configuration.\n",
    "    aligned = pd.Series(index=ff_factors.index, dtype=float, data=np.nan)\n",
    "    for date in returns.index:\n",
    "        if date in aligned.index:\n",
    "            aligned[date] = returns[date]\n",
    "    aligned = aligned.dropna()\n",
    "    \n",
    "    # Calculate excess returns (subtract risk-free rate)\n",
    "    rf = ff_factors.loc[aligned.index, 'RF']\n",
    "    excess = aligned - rf\n",
    "\n",
    "    # Build a DataFrame with excess returns and the FF factors\n",
    "    etf_df = pd.DataFrame({\n",
    "        'Stock_Excess_Return': excess,\n",
    "        'Mkt_RF': ff_factors.loc[aligned.index, 'Mkt_RF'],\n",
    "        'SMB':    ff_factors.loc[aligned.index, 'SMB'],\n",
    "        'HML':    ff_factors.loc[aligned.index, 'HML'],\n",
    "        'RF':     rf\n",
    "    }).dropna()\n",
    "    \n",
    "    etf_dataframes[ticker] = etf_df\n",
    "    print(f\"    {len(etf_df)} months loaded\")\n",
    "\n",
    "# Find overlapping period across all ETFs\n",
    "common_idx = etf_dataframes['SPY'].index\n",
    "for ticker in etf_tickers:\n",
    "    common_idx = common_idx.intersection(etf_dataframes[ticker].index)\n",
    "\n",
    "# Trim all DataFrames to the common period\n",
    "for ticker in etf_tickers:\n",
    "    etf_dataframes[ticker] = etf_dataframes[ticker].loc[common_idx]\n",
    "\n",
    "# Set the primary DataFrame to SPY for the main tutorial analysis\n",
    "df = etf_dataframes['SPY'].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA LOADING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOverlapping period: {common_idx[0].date()} to {common_idx[-1].date()}\")\n",
    "print(f\"Number of months:   {len(common_idx)}\")\n",
    "print(f\"\\nETFs loaded:\")\n",
    "for ticker, label in etf_tickers.items():\n",
    "    print(f\"  - {label}\")\n",
    "print(f\"\\nPrimary analysis ETF: SPY (S&P 500)\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst 10 rows (returns in decimal form, e.g. 0.05 = 5%):\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nData shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d9e08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Descriptive Statistics — Understanding Your Data\n",
    "\n",
    "Before building a regression model, we need to understand the basic properties of our data. Descriptive statistics summarize large datasets into a few key numbers — mean, standard deviation, percentiles, and correlations.\n",
    "\n",
    "> **Prerequisite recap:** If any of the formulas below are unfamiliar (e.g., Bessel's correction, the difference between $n$ and $n-1$, or how sample covariance relates to population covariance), see the [Statistical Foundations](01_Statistical_Foundations.ipynb), Sections 1–2, where every formula is derived and verified on a 5-observation dataset.\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Statistic | Formula | Financial Meaning |\n",
    "|-----------|---------|-------------------|\n",
    "| Mean | $\\bar{x} = \\frac{1}{n}\\sum x_i$ | Average monthly return |\n",
    "| Std Dev | $s = \\sqrt{\\frac{1}{n-1}\\sum(x_i - \\bar{x})^2}$ | Volatility / risk |\n",
    "| Correlation | $r_{xy} = \\text{Cov}(x,y) / (s_x s_y)$ | Co-movement (−1 to +1) |\n",
    "\n",
    "**Key things to look for:**\n",
    "- **Mkt_RF mean × 12** gives a rough annualized market premium\n",
    "- **Std Dev × $\\sqrt{12}$** gives an annualized volatility estimate (typical US equity: 15–20%)\n",
    "- **Low correlations between factors** (SMB, HML, Mkt-RF) confirm they capture different dimensions of risk — this is by construction\n",
    "\n",
    "### Now let's examine our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e316e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptive statistics\n",
    "print(\"\\nDESCRIPTIVE STATISTICS (All returns in decimal form)\\n\")\n",
    "print(df.describe())\n",
    "\n",
    "# Calculate correlations for numeric columns only\n",
    "print(\"\\n\")\n",
    "print(\"CORRELATION MATRIX\\n\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df[numeric_cols].corr().round(4)\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualize: Correlation heatmap + factor distributions\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5.5))\n",
    "\n",
    "# --- Left: Correlation heatmap ---\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            mask=mask, square=True, ax=axes[0], vmin=-1, vmax=1,\n",
    "            linewidths=0.5)\n",
    "axes[0].set_title('Correlation Matrix', fontweight='bold', fontsize=13)\n",
    "\n",
    "# --- Right: Distribution of each factor + stock excess return ---\n",
    "plot_cols = [c for c in ['Mkt_RF', 'SMB', 'HML', 'Stock_Excess_Return'] if c in df.columns]\n",
    "colors = ['#2c3e50', '#e74c3c', '#2ecc71', '#3498db']\n",
    "for col, color in zip(plot_cols, colors):\n",
    "    axes[1].hist(df[col].dropna(), bins=30, alpha=0.5, color=color, label=col, density=True)\n",
    "axes[1].set_title('Return Distributions (Monthly)', fontweight='bold', fontsize=13)\n",
    "axes[1].set_xlabel('Monthly Return')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc22c7",
   "metadata": {},
   "source": [
    "### Interpreting the Descriptive Statistics\n",
    "\n",
    "The exact numbers you see will depend on the data downloaded from Yahoo Finance. Here are the key things to look for:\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "**Count (n)**: Check how many months of data we have. With ~168 months (2010–2023), we have a solid sample for reliable inference.\n",
    "\n",
    "**Mean (Average Returns)**:\n",
    "- **Mkt_RF**: The average monthly market excess return. Multiply by 12 for a rough annualized figure (or compute $(1 + \\bar{r})^{12} - 1$ for the compound annualized return).\n",
    "- **Stock_Excess_Return**: Since SPY tracks the broad US market, its mean excess return should be close to the Mkt_RF mean.\n",
    "\n",
    "**Standard Deviation (Volatility)**:\n",
    "- **Mkt_RF** and **Stock_Excess_Return**: For a broad US market ETF like SPY, these should be similar. Multiply monthly std dev by $\\sqrt{12}$ for an annualized approximation.\n",
    "- A typical annualized US equity volatility is around 15–20%.\n",
    "\n",
    "**Range (Min to Max)**:\n",
    "- Look for extreme months (e.g., the COVID crash in March 2020 will appear as a large negative).\n",
    "- Even diversified ETFs experience significant monthly swings.\n",
    "\n",
    "**The Correlation Matrix** shows how variables move together (scale: -1 to +1):\n",
    "\n",
    "**Between Factors**:\n",
    "- Mkt-RF vs SMB: Typically weak to moderate (small and large caps move somewhat together)\n",
    "- Mkt-RF vs HML: Typically near zero (market and value effects are largely independent)\n",
    "- SMB vs HML: Typically near zero (confirming they capture different risks)\n",
    "\n",
    "**SPY's Correlations**:\n",
    "- Mkt-RF vs SPY: Should be **very high** (close to 1), since SPY tracks the US market that the Mkt-RF factor is derived from. This is exactly what we expect!\n",
    "- SMB vs SPY: Should be near zero or slightly negative — a large-cap-heavy index has limited small-cap exposure.\n",
    "- HML vs SPY: Should be near zero — a market-cap-weighted index has no particular value tilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation of statistics for Mkt-RF\n",
    "mkt_returns = df['Mkt_RF'].values\n",
    "n = len(mkt_returns)\n",
    "\n",
    "# Calculate mean\n",
    "mean = mkt_returns.sum() / n\n",
    "\n",
    "# Calculate standard deviation\n",
    "deviations = mkt_returns - mean\n",
    "squared_deviations = deviations ** 2\n",
    "variance = squared_deviations.sum() / (n - 1)\n",
    "std_dev = np.sqrt(variance)\n",
    "\n",
    "# Calculate quartiles and range\n",
    "min_val = mkt_returns.min()\n",
    "q1 = np.percentile(mkt_returns, 25)\n",
    "median = np.percentile(mkt_returns, 50)\n",
    "q3 = np.percentile(mkt_returns, 75)\n",
    "max_val = mkt_returns.max()\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Results\n",
    "print(\"Manual Calculation Results (Mkt-RF):\")\n",
    "print(f\"Count: {n}\")\n",
    "print(f\"Mean: {mean:.6f} ({mean:.4%} monthly, {(1 + mean)**12 - 1:.4%} annualized)\")\n",
    "print(f\"Std Dev: {std_dev:.6f} ({std_dev:.4%} monthly, {std_dev * np.sqrt(12):.2%} annualized)\")\n",
    "print(f\"Min: {min_val:.6f} ({min_val:.2%})\")\n",
    "print(f\"Q1: {q1:.6f} ({q1:.2%})\")\n",
    "print(f\"Median: {median:.6f} ({median:.2%})\")\n",
    "print(f\"Q3: {q3:.6f} ({q3:.2%})\")\n",
    "print(f\"Max: {max_val:.6f} ({max_val:.2%})\")\n",
    "print(f\"IQR: {iqr:.6f}\")\n",
    "\n",
    "# Manual calculation of correlation\n",
    "mkt = df['Mkt_RF'].values\n",
    "stock = df['Stock_Excess_Return'].values\n",
    "mkt_mean = mkt.mean()\n",
    "stock_mean = stock.mean()\n",
    "cov_numerator = ((mkt - mkt_mean) * (stock - stock_mean)).sum()\n",
    "covariance = cov_numerator / (n - 1)\n",
    "mkt_std = df['Mkt_RF'].std()\n",
    "stock_std = df['Stock_Excess_Return'].std()\n",
    "correlation = covariance / (mkt_std * stock_std)\n",
    "\n",
    "print(f\"\\nManual Correlation (Mkt-RF vs Stock_Excess_Return):\")\n",
    "print(f\"Covariance: {covariance:.6f}\")\n",
    "print(f\"Correlation: {correlation:.6f}\")\n",
    "print(f\"Verification (pandas): {df['Mkt_RF'].corr(df['Stock_Excess_Return']):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d1f41",
   "metadata": {},
   "source": [
    "### Understanding These Manual Calculations\n",
    "\n",
    "The code above recalculates statistics \"from scratch\" using the raw formulas: mean = sum/count, standard deviation via $\\sqrt{\\sum(x_i - \\bar{x})^2 / (n-1)}$, covariance, and correlation. This is the same approach used on 5 data points in the [Statistical Foundations](01_Statistical_Foundations.ipynb) notebook (Sections 1–2) — now applied at scale to real market data. The pandas `.describe()` and `.corr()` methods compute exactly the same quantities.\n",
    "\n",
    "**Why These Matter for the Fama-French Model:**\n",
    "1. **Mean** — expected excess returns, i.e., compensation for taking risk\n",
    "2. **Standard deviation** — volatility / risk level of each factor\n",
    "3. **Correlation** — whether factors capture *different* risks (low correlation = good: the factors are not redundant)\n",
    "\n",
    "For SPY vs Mkt-RF specifically, we expect correlation close to 1.0, since SPY tracks the very market that the Mkt-RF factor is derived from.\n",
    "\n",
    "Let's now use these factors in a regression model to explain stock returns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc702baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the time series of returns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "# ETF Returns (one subplot per ETF)\n",
    "colors_etf = {'SPY': 'darkblue', 'IWN': 'darkorange', 'VTV': 'darkgreen'}\n",
    "for ax, (ticker, label) in zip(axes[:, 0], etf_tickers.items()):\n",
    "    etf_df = etf_dataframes[ticker]\n",
    "    ax.plot(etf_df.index, etf_df['Stock_Excess_Return']*100,\n",
    "            linewidth=1.5, color=colors_etf[ticker])\n",
    "    ax.set_title(f'{label} - Excess Returns', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Return (%)', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Fama-French Factors\n",
    "axes[0, 1].plot(df.index, df['Mkt_RF']*100, linewidth=1.5, color='darkred')\n",
    "axes[0, 1].set_title('Market Risk Premium (Mkt-RF)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Return (%)', fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(df.index, df['SMB']*100, linewidth=1.5, color='teal')\n",
    "axes[1, 1].set_title('SMB Factor (Size Effect)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Return (%)', fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2, 1].plot(df.index, df['HML']*100, linewidth=1.5, color='purple')\n",
    "axes[2, 1].set_title('HML Factor (Value Effect)', fontsize=11, fontweight='bold')\n",
    "axes[2, 1].set_ylabel('Return (%)', fontsize=10)\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fama_french_timeseries.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Time series visualization created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e02ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Understanding Excess Returns and Regression Concepts\n",
    "\n",
    "### What are Excess Returns?\n",
    "\n",
    "In the Fama-French model, we don't use raw returns directly. Instead, we use **excess returns**, which are the returns on an investment minus the **risk-free rate**.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\\text{Excess Return} = \\text{Actual Return} - \\text{Risk-Free Rate}$$\n",
    "\n",
    "**Why do this?** The risk-free rate represents the return you could get with zero risk (like Treasury bonds). By subtracting it, we're asking: \"How much extra return does this investment provide beyond what a safe investment would give?\"\n",
    "\n",
    "In our data:\n",
    "- `Stock_Excess_Return` = The excess return on our ETF (already calculated)\n",
    "- `Mkt_RF` = The market excess return (Market return minus risk-free rate)\n",
    "- `SMB` and `HML` = Factor returns (already in excess form)\n",
    "\n",
    "### Multiple Linear Regression: The Concept\n",
    "\n",
    "We want to estimate a relationship like this:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon$$\n",
    "\n",
    "In our case:\n",
    "$$R_i - R_f = \\alpha + \\beta_1 (R_m - R_f) + \\beta_2 \\cdot SMB + \\beta_3 \\cdot HML + \\epsilon$$\n",
    "\n",
    "**What this means:**\n",
    "- The ETF's excess return ($y$) depends on three factors ($x_1, x_2, x_3$)\n",
    "- We want to find the coefficients ($\\beta_0, \\beta_1, \\beta_2, \\beta_3$) that best fit the data\n",
    "- $\\epsilon$ is the \"leftover\" part we can't explain—the error term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8fb53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "### Applying the Normal Equations to Real Factor Data\n",
    "\n",
    "OLS finds the coefficients that minimize the sum of squared errors. In matrix form:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "> **Full derivation:** See [Statistical Foundations](01_Statistical_Foundations.ipynb) (Section 5) for the scalar and matrix derivations of the normal equations with hand-checkable examples. Here we apply the same formula to real Fama-French data.\n",
    "\n",
    "For our 3-factor model, the design matrix is:\n",
    "\n",
    "$$\\mathbf{X} = \\begin{pmatrix}\n",
    "1 & (R_m - R_f)_1 & SMB_1 & HML_1 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & (R_m - R_f)_n & SMB_n & HML_n\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The first column of 1's gives us the intercept ($\\alpha$). The code below walks through each matrix operation step by step on the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's manually compute the OLS estimates using the normal equations\n",
    "# to show exactly how this works in practice\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MANUAL OLS CALCULATION USING NORMAL EQUATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Construct the design matrix X\n",
    "# X has shape (n, 4): [intercept, Mkt_RF, SMB, HML]\n",
    "X = np.column_stack([\n",
    "    np.ones(len(df)),           # Column of 1's for the intercept\n",
    "    df['Mkt_RF'].values,         # Market risk premium\n",
    "    df['SMB'].values,            # SMB factor\n",
    "    df['HML'].values             # HML factor\n",
    "])\n",
    "\n",
    "# Step 2: Extract the dependent variable (stock excess returns)\n",
    "y = df['Stock_Excess_Return'].values\n",
    "\n",
    "print(f\"\\nDesign matrix X shape: {X.shape}\")\n",
    "print(f\"Dependent variable y shape: {y.shape}\")\n",
    "print(f\"\\nFirst 5 rows of X:\")\n",
    "print(f\"   [Intercept]  [Mkt_RF]  [SMB]    [HML]\")\n",
    "print(X[:5])\n",
    "\n",
    "# Step 3: Calculate X^T (transpose)\n",
    "X_T = X.T\n",
    "print(f\"\\nX^T shape: {X_T.shape}\")\n",
    "\n",
    "# Step 4: Calculate X^T * X\n",
    "XTX = X_T @ X\n",
    "print(f\"\\nX^T * X (shape {XTX.shape}):\")\n",
    "print(XTX)\n",
    "\n",
    "# Step 5: Calculate (X^T * X)^-1\n",
    "XTX_inv = np.linalg.inv(XTX)\n",
    "print(f\"\\n(X^T * X)^-1:\")\n",
    "print(XTX_inv)\n",
    "\n",
    "# Step 6: Calculate X^T * y\n",
    "XTy = X_T @ y\n",
    "print(f\"\\nX^T * y:\")\n",
    "print(XTy)\n",
    "\n",
    "# Step 7: Calculate beta_hat = (X^T * X)^-1 * X^T * y\n",
    "beta_hat = XTX_inv @ XTy\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ESTIMATED COEFFICIENTS (β̂)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Alpha (α̂):          {beta_hat[0]:.6f} (intercept)\")\n",
    "print(f\"Beta MKT (β̂_MKT):   {beta_hat[1]:.6f} (market sensitivity)\")\n",
    "print(f\"Beta SMB (β̂_SMB):   {beta_hat[2]:.6f} (size sensitivity)\")\n",
    "print(f\"Beta HML (β̂_HML):   {beta_hat[3]:.6f} (value sensitivity)\")\n",
    "\n",
    "# Remember: These are the \"true\" parameters we used to generate the data\n",
    "# We will verify these manual estimates match the statsmodels output in the next section.\n",
    "print(\"\\nNote: We will compare these manual estimates with the statsmodels\")\n",
    "print(\"output in the next section to confirm they are identical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fe764",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Fitting the Model with `statsmodels`\n",
    "\n",
    "In Section 5 we computed the OLS coefficients manually using the normal equations. In practice, we use a statistical library — `statsmodels` — which does the same calculation and also computes many additional diagnostics automatically.\n",
    "\n",
    "### What Does `statsmodels` Do?\n",
    "\n",
    "When we call `sm.OLS(y, X).fit()`, the library:\n",
    "\n",
    "1. **Computes the OLS estimates** $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$ — exactly what we did by hand.\n",
    "2. **Estimates the error variance** $\\hat{\\sigma}^2 = \\frac{RSS}{n - k}$ from the residuals.\n",
    "3. **Calculates many additional statistics** — standard errors, t-statistics, p-values, $R^2$, and more.\n",
    "\n",
    "For now, we'll only verify that `statsmodels` produces the **same coefficients** as our manual calculation. This is an important sanity check: if both methods agree, we know our math is correct.\n",
    "\n",
    "The additional statistics (R-squared, standard errors, t-statistics, p-values, etc.) will be introduced **one at a time** in the sections that follow, so you can understand each one properly before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6599f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Fama-French 3-factor regression model using statsmodels\n",
    "\n",
    "# Prepare the data\n",
    "y = df['Stock_Excess_Return'].values\n",
    "X_factors = df[['Mkt_RF', 'SMB', 'HML']].values\n",
    "\n",
    "# Add constant (intercept) term\n",
    "X_with_const = sm.add_constant(X_factors)\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y, X_with_const)\n",
    "results = model.fit()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Verify: do the statsmodels coefficients match our manual calculation?\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICATION: MANUAL vs STATSMODELS COEFFICIENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "labels = ['Alpha (intercept)', 'Beta_MKT', 'Beta_SMB', 'Beta_HML']\n",
    "print(f\"\\n{'Coefficient':<22s}  {'Manual':>12s}  {'statsmodels':>12s}  {'Match?':>8s}\")\n",
    "print(\"-\"*60)\n",
    "for i, name in enumerate(labels):\n",
    "    manual = beta_hat[i]\n",
    "    sm_coef = results.params[i]\n",
    "    match = np.isclose(manual, sm_coef)\n",
    "    print(f\"{name:<22s}  {manual:12.6f}  {sm_coef:12.6f}  {'Yes' if match else 'NO':>8s}\")\n",
    "\n",
    "print(f\"\\nAll coefficients match: {np.allclose(beta_hat, results.params)}\")\n",
    "print(\"\\nThe statsmodels results object also contains standard errors,\")\n",
    "print(\"t-statistics, p-values, R-squared, and many other diagnostics.\")\n",
    "print(\"We will explore each of these in the sections that follow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualize the regression: fitted vs actual + scatter against main factor\n",
    "# ============================================================================\n",
    "# Seeing the regression visually builds intuition that tables alone cannot.\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# Panel 1: Fitted vs Actual (the classic regression-quality scatter)\n",
    "fitted = results.fittedvalues * 100   # convert to %\n",
    "actual = y * 100\n",
    "axes[0].scatter(fitted, actual, s=20, alpha=0.55, color='steelblue', edgecolors='white', linewidth=0.3)\n",
    "lo, hi = min(fitted.min(), actual.min()), max(fitted.max(), actual.max())\n",
    "axes[0].plot([lo, hi], [lo, hi], color='red', linewidth=1.5, linestyle='--', label='Perfect fit')\n",
    "axes[0].set_xlabel('Fitted return (%)', fontsize=11)\n",
    "axes[0].set_ylabel('Actual return (%)', fontsize=11)\n",
    "axes[0].set_title('Actual vs Fitted Returns\\n(Closer to the red line = better fit)', fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Excess Return vs Mkt-RF (the dominant factor)\n",
    "mkt_vals = df['Mkt_RF'].values * 100\n",
    "axes[1].scatter(mkt_vals, actual, s=20, alpha=0.55, color='darkorange', edgecolors='white', linewidth=0.3)\n",
    "# Simple bivariate OLS line for visual reference\n",
    "slope_simple = np.polyfit(mkt_vals, actual, 1)\n",
    "x_line = np.linspace(mkt_vals.min(), mkt_vals.max(), 100)\n",
    "axes[1].plot(x_line, np.polyval(slope_simple, x_line), color='red', linewidth=1.5, linestyle='--')\n",
    "axes[1].set_xlabel('Market Excess Return, Mkt-RF (%)', fontsize=11)\n",
    "axes[1].set_ylabel('SPY Excess Return (%)', fontsize=11)\n",
    "axes[1].set_title('SPY vs Market Factor\\n(Slope ≈ market beta)', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Residuals vs Fitted (preview of diagnostics)\n",
    "resid = results.resid * 100\n",
    "axes[2].scatter(fitted, resid, s=20, alpha=0.55, color='seagreen', edgecolors='white', linewidth=0.3)\n",
    "axes[2].axhline(0, color='red', linewidth=1.5, linestyle='--')\n",
    "axes[2].set_xlabel('Fitted return (%)', fontsize=11)\n",
    "axes[2].set_ylabel('Residual (%)', fontsize=11)\n",
    "axes[2].set_title('Residuals vs Fitted\\n(Look for patterns — ideally none)', fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left:   Points near the 45° line → model captures most variation.\")\n",
    "print(\"Center: The steep slope confirms SPY moves nearly 1-for-1 with the market.\")\n",
    "print(\"Right:  No visible pattern in residuals → model specification looks reasonable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83c1ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Model Performance Metrics\n",
    "\n",
    "### Understanding R-squared and Other Fit Statistics\n",
    "\n",
    "Before diving into the individual coefficients, let's assess how well the model fits the data overall. These metrics don't depend on the standard error method, so they're safe to interpret right away.\n",
    "\n",
    "### The R-squared Formula\n",
    "\n",
    "$$R^2 = 1 - \\frac{RSS}{TSS}$$\n",
    "\n",
    "**Where:**\n",
    "- $RSS$ (Residual Sum of Squares) = $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ = Sum of squared residuals\n",
    "- $TSS$ (Total Sum of Squares) = $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ = Total variation in $y$\n",
    "- $\\bar{y}$ = Mean of the dependent variable\n",
    "\n",
    "**Intuition:**\n",
    "- $TSS$ measures how much $y$ varies around its mean (total variation to explain)\n",
    "- $RSS$ measures how much unexplained variation remains after fitting the model\n",
    "- $R^2$ tells us what fraction of the total variation is explained by the model\n",
    "- $R^2$ ranges from 0 to 1; higher is better\n",
    "\n",
    "### Adjusted R-squared\n",
    "\n",
    "The problem with $R^2$ is that adding more variables always increases it (even bad variables). Adjusted $R^2$ penalizes for the number of variables:\n",
    "\n",
    "$$\\bar{R}^2 = 1 - \\frac{RSS/(n-k)}{TSS/(n-1)} = 1 - \\frac{(1-R^2)(n-1)}{n-k}$$\n",
    "\n",
    "**Where:**\n",
    "- $n$ = number of observations\n",
    "- $k$ = number of parameters (including intercept)\n",
    "- The denominator $(n-k)$ is called \"degrees of freedom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model performance metrics\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL PERFORMANCE METRICS - MANUAL CALCULATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions and residuals\n",
    "y_actual = df['Stock_Excess_Return'].values\n",
    "y_pred = np.asarray(results.fittedvalues)\n",
    "residuals = np.asarray(results.resid)\n",
    "\n",
    "# Calculate means\n",
    "y_mean = np.mean(y_actual)\n",
    "\n",
    "# Calculate sums of squares\n",
    "TSS = np.sum((y_actual - y_mean)**2)           # Total Sum of Squares\n",
    "RSS = np.sum(residuals**2)                     # Residual Sum of Squares\n",
    "ESS = np.sum((y_pred - y_mean)**2)             # Explained Sum of Squares\n",
    "\n",
    "print(f\"\\nNOTE: TSS = RSS + ESS (always true)\")\n",
    "print(f\"TSS = {TSS:.8f}\")\n",
    "print(f\"RSS = {RSS:.8f}\")\n",
    "print(f\"ESS = {ESS:.8f}\")\n",
    "print(f\"TSS = RSS + ESS? {np.isclose(TSS, RSS + ESS)}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "R2 = 1 - (RSS / TSS)\n",
    "print(f\"\\n1. R-SQUARED (R²)\")\n",
    "print(f\"   Formula: R² = 1 - (RSS / TSS)\")\n",
    "print(f\"   Calculation: R² = 1 - ({RSS:.6f} / {TSS:.6f})\")\n",
    "print(f\"   R² = {R2:.6f}\")\n",
    "print(f\"   Interpretation: {R2*100:.2f}% of the variation in stock returns is\")\n",
    "print(f\"   explained by the three Fama-French factors.\")\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = len(y_actual)\n",
    "k = 4  # 4 parameters: alpha, beta_mkt, beta_smb, beta_hml\n",
    "adj_R2 = 1 - ((1 - R2) * (n - 1) / (n - k))\n",
    "print(f\"\\n2. ADJUSTED R-SQUARED (R̄²)\")\n",
    "print(f\"   Formula: R̄² = 1 - ((1 - R²) × (n-1) / (n-k))\")\n",
    "print(f\"   Where n = {n} observations, k = {k} parameters\")\n",
    "print(f\"   Calculation: R̄² = 1 - ((1 - {R2:.4f}) × {n-1} / {n-k})\")\n",
    "print(f\"   R̄² = {adj_R2:.6f}\")\n",
    "print(f\"   Interpretation: After adjusting for the number of variables,\")\n",
    "print(f\"   {adj_R2*100:.2f}% of variation is explained.\")\n",
    "\n",
    "# Calculate standard deviation of regression\n",
    "sigma_hat_sq = RSS / (n - k)\n",
    "sigma_hat = np.sqrt(sigma_hat_sq)\n",
    "print(f\"\\n3. RESIDUAL STANDARD ERROR (σ̂)\")\n",
    "print(f\"   Formula: σ̂² = RSS / (n - k)\")\n",
    "print(f\"   Calculation: σ̂² = {RSS:.6f} / {n - k}\")\n",
    "print(f\"   σ̂² = {sigma_hat_sq:.8f}\")\n",
    "print(f\"   σ̂ = {sigma_hat:.6f}\")\n",
    "print(f\"   Interpretation: On average, predictions are off by {sigma_hat:.6f}\")\n",
    "print(f\"   ({sigma_hat*100:.4f}% per month in this case).\")\n",
    "\n",
    "# Calculate F-statistic\n",
    "MSR = ESS / (k - 1)  # Mean Squared Regression\n",
    "MSE = RSS / (n - k)  # Mean Squared Error\n",
    "F_stat = MSR / MSE\n",
    "# Calculate p-value for F-statistic\n",
    "from scipy.stats import f\n",
    "p_value_F = 1 - f.cdf(F_stat, k-1, n-k)\n",
    "print(f\"\\n4. F-STATISTIC\")\n",
    "print(f\"   Formula: F = (ESS / (k-1)) / (RSS / (n-k)) = MSR / MSE\")\n",
    "print(f\"   MSR = {MSR:.8f}, MSE = {MSE:.8f}\")\n",
    "print(f\"   F-statistic = {F_stat:.4f}\")\n",
    "print(f\"   p-value = {p_value_F:.2e}\")\n",
    "print(f\"   Interpretation: This tests the null hypothesis that ALL coefficients\")\n",
    "print(f\"   (except intercept) equal zero. If p-value < 0.05, we reject this,\")\n",
    "print(f\"   meaning the model IS statistically significant.\")\n",
    "if p_value_F < 0.05:\n",
    "    print(f\"   The model IS statistically significant (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"   The model is NOT statistically significant\")\n",
    "\n",
    "# Compare with statsmodels values\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON WITH STATSMODELS OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Our R²:               {R2:.6f}\")\n",
    "print(f\"Statsmodels R²:       {results.rsquared:.6f}\")\n",
    "print(f\"Match? {np.isclose(R2, results.rsquared)}\")\n",
    "print(f\"\\nOur Adjusted R²:      {adj_R2:.6f}\")\n",
    "print(f\"Statsmodels Adj R²:   {results.rsquared_adj:.6f}\")\n",
    "print(f\"Match? {np.isclose(adj_R2, results.rsquared_adj)}\")\n",
    "print(f\"\\nOur F-statistic:      {F_stat:.4f}\")\n",
    "print(f\"Statsmodels F-stat:   {results.fvalue:.4f}\")\n",
    "print(f\"Match? {np.isclose(F_stat, results.fvalue)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualize: Model performance metrics\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Left: Variance decomposition (TSS = ESS + RSS) ---\n",
    "ax = axes[0]\n",
    "labels = ['Explained\\n(ESS)', 'Residual\\n(RSS)']\n",
    "sizes = [ESS, RSS]\n",
    "colors_pie = ['#3498db', '#e74c3c']\n",
    "wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors_pie,\n",
    "                                   autopct='%1.1f%%', startangle=90,\n",
    "                                   textprops={'fontsize': 11})\n",
    "for at in autotexts:\n",
    "    at.set_fontweight('bold')\n",
    "ax.set_title(f'Variance Decomposition (R² = {R2:.4f})', fontweight='bold', fontsize=12)\n",
    "\n",
    "# --- Right: Key metrics summary ---\n",
    "ax2 = axes[1]\n",
    "metric_names = ['R²', 'Adj R²', f'σ̂ (×100)']\n",
    "metric_vals = [R2, adj_R2, sigma_hat * 100]\n",
    "bar_colors = ['#3498db', '#2980b9', '#e67e22']\n",
    "bars = ax2.barh(metric_names, metric_vals, color=bar_colors, edgecolor='white', height=0.5)\n",
    "for bar, val in zip(bars, metric_vals):\n",
    "    ax2.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "             f'{val:.4f}', va='center', fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_title('Model Performance Metrics', fontweight='bold', fontsize=12)\n",
    "ax2.set_xlim(0, max(metric_vals) * 1.25)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2be18f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Validating Regression Assumptions and Diagnostics\n",
    "\n",
    "### Why This Section Matters\n",
    "\n",
    "The regression in Section 6 gave us coefficient estimates ($\\hat{\\alpha}$, $\\hat{\\beta}_{MKT}$, etc.) and the performance metrics in Section 7 told us how well the model fits. But those numbers come with a hidden catch: **the standard errors, t-statistics, and p-values printed alongside the coefficients are only trustworthy if certain assumptions about the error term hold.**\n",
    "\n",
    "If those assumptions fail — and they often do for financial data — then:\n",
    "- The coefficient estimates themselves are typically still fine (unbiased and consistent).\n",
    "- But the **uncertainty measures are wrong**: standard errors tend to be too small, t-statistics too large, and p-values too optimistic. In other words, you might conclude a factor is statistically significant when it really isn't.\n",
    "\n",
    "> **Connection to earlier notebooks:** The [Time-Series Foundations](02_Time_Series_Foundations.ipynb) notebook showed empirically that financial return data is approximately stationary (Section 2), has weak autocorrelation but strong volatility clustering (Sections 3–4), and satisfies mixing conditions (Section 5). The summary \"assumption stack\" (Section 6) previewed exactly which assumptions hold and which fail. Here we check the same assumptions directly on the Fama-French regression residuals.\n",
    "\n",
    "This section checks the assumptions; the **next** section (Section 9) will show how to fix things when they fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe33847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diagnostic plots\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Get residuals and fitted values\n",
    "residuals = np.asarray(results.resid)\n",
    "fitted_values = np.asarray(results.fittedvalues)\n",
    "y_actual = df['Stock_Excess_Return'].values\n",
    "\n",
    "# 1. Residuals vs Fitted Values (checks linearity and homoscedasticity)\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "ax1.scatter(fitted_values, residuals, alpha=0.6, s=50)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Fitted Values')\n",
    "ax1.set_ylabel('Residuals')\n",
    "ax1.set_title('Residuals vs Fitted Values\\n(Should show random scatter around 0)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Q-Q Plot (checks normality of residuals)\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot\\n(Points should lie on diagonal line)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scale-Location Plot (checks homoscedasticity)\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "standardized_residuals = residuals / np.std(residuals)\n",
    "ax3.scatter(fitted_values, np.sqrt(np.abs(standardized_residuals)), alpha=0.6, s=50)\n",
    "ax3.set_xlabel('Fitted Values')\n",
    "ax3.set_ylabel('|Standardized Residuals|')\n",
    "ax3.set_title('Scale-Location Plot\\n(Should show random scatter)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Histogram of residuals (checks normality)\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "ax4.hist(residuals, bins=30, edgecolor='black', alpha=0.7, density=True)\n",
    "# Overlay normal distribution\n",
    "mu, sigma = residuals.mean(), residuals.std()\n",
    "x = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "ax4.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal Distribution')\n",
    "ax4.set_xlabel('Residuals')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Histogram of Residuals\\n(Should look bell-shaped)', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Actual vs Fitted\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "ax5.scatter(y_actual, fitted_values, alpha=0.6, s=50)\n",
    "min_val = min(y_actual.min(), fitted_values.min())\n",
    "max_val = max(y_actual.max(), fitted_values.max())\n",
    "ax5.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Fit')\n",
    "ax5.set_xlabel('Actual Values')\n",
    "ax5.set_ylabel('Fitted Values')\n",
    "ax5.set_title('Actual vs Fitted Values\\n(Should lie on diagonal line)', fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Residuals over time (checks independence)\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.plot(range(len(residuals)), residuals, marker='o', linestyle='-', alpha=0.6, markersize=4)\n",
    "ax6.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax6.set_xlabel('Observation Number')\n",
    "ax6.set_ylabel('Residuals')\n",
    "ax6.set_title('Residuals Over Time\\n(Should show no pattern)', fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fama_french_diagnostics.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Diagnostic plots created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a9aa10",
   "metadata": {},
   "source": [
    "### Formal Statistical Tests for Assumptions\n",
    "\n",
    "Visual inspection gives us intuition, but formal statistical tests provide rigorous evidence. Each test below has a null hypothesis ($H_0$) representing the \"ideal\" scenario and an alternative that flags a violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23429120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FORMAL TESTS FOR REGRESSION ASSUMPTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Normality Test: Jarque-Bera\n",
    "print(\"\\n1. JARQUE-BERA TEST FOR NORMALITY\")\n",
    "print(\"   Null Hypothesis (H0): Residuals are normally distributed\")\n",
    "print(\"   Alternative Hypothesis (H1): Residuals are NOT normally distributed\")\n",
    "print(\"   Caveat: With ~100+ observations, JB can over-reject for financial\")\n",
    "print(\"   data that has mild fat tails (excess kurtosis), even when the\")\n",
    "print(\"   departure from normality is too small to meaningfully affect OLS.\")\n",
    "print(\"   This is one reason we use robust standard errors regardless.\")\n",
    "jb_result = jarque_bera(residuals)\n",
    "jb_statistic, jb_pvalue = jb_result[0], jb_result[1]\n",
    "print(f\"   Test Statistic: {jb_statistic:.6f}\")\n",
    "print(f\"   p-value: {jb_pvalue:.6f}\")\n",
    "if jb_pvalue > 0.05:\n",
    "    print(f\"   CONCLUSION: We fail to reject H0 (p > 0.05)\")\n",
    "    print(f\"     Residuals appear to be normally distributed.\")\n",
    "else:\n",
    "    print(f\"   CONCLUSION: We reject H0 (p < 0.05)\")\n",
    "    print(f\"     Residuals appear to NOT be normally distributed.\")\n",
    "\n",
    "# 2. Breusch-Pagan Test for Heteroscedasticity\n",
    "print(\"\\n2. BREUSCH-PAGAN TEST FOR HOMOSCEDASTICITY\")\n",
    "print(\"   Null Hypothesis (H0): Constant variance of residuals (homoscedasticity)\")\n",
    "print(\"   Alternative Hypothesis (H1): Variance is NOT constant (heteroscedasticity)\")\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "bp_test = het_breuschpagan(residuals, X_with_const)\n",
    "bp_statistic, bp_pvalue = bp_test[0], bp_test[1]\n",
    "print(f\"   Test Statistic: {bp_statistic:.6f}\")\n",
    "print(f\"   p-value: {bp_pvalue:.6f}\")\n",
    "if bp_pvalue > 0.05:\n",
    "    print(f\"   CONCLUSION: We fail to reject H0 (p > 0.05)\")\n",
    "    print(f\"     The variance appears to be constant across fitted values.\")\n",
    "else:\n",
    "    print(f\"   CONCLUSION: We reject H0 (p < 0.05)\")\n",
    "    print(f\"     There appears to be heteroscedasticity.\")\n",
    "\n",
    "# 3. Durbin-Watson Test for Autocorrelation\n",
    "print(\"\\n3. DURBIN-WATSON TEST FOR AUTOCORRELATION\")\n",
    "print(\"   Tests for correlation between consecutive residuals\")\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_statistic = durbin_watson(residuals)\n",
    "print(f\"   Durbin-Watson Statistic: {dw_statistic:.6f}\")\n",
    "print(f\"   Interpretation:\")\n",
    "print(f\"      DW ~ 2: No autocorrelation (ideal)\")\n",
    "print(f\"      DW < 2: Positive autocorrelation\")\n",
    "print(f\"      DW > 2: Negative autocorrelation\")\n",
    "if 1.5 < dw_statistic < 2.5:\n",
    "    print(f\"   CONCLUSION: No significant autocorrelation detected.\")\n",
    "else:\n",
    "    print(f\"   CONCLUSION: Possible autocorrelation detected (DW = {dw_statistic:.4f})\")\n",
    "\n",
    "# 4. Variance Inflation Factor (VIF) for Multicollinearity\n",
    "print(\"\\n4. VARIANCE INFLATION FACTOR (VIF) FOR MULTICOLLINEARITY\")\n",
    "print(\"   VIF measures how much the variance of a coefficient is\")\n",
    "print(\"   inflated due to correlation with other variables.\")\n",
    "print(\"   Interpretation: VIF > 5 suggests multicollinearity problem\")\n",
    "print()\n",
    "vif_values = []\n",
    "for i in range(1, X_with_const.shape[1]):  # Skip the constant\n",
    "    vif = variance_inflation_factor(X_with_const, i)\n",
    "    var_name = ['Mkt_RF', 'SMB', 'HML'][i-1]\n",
    "    vif_values.append((var_name, vif))\n",
    "    print(f\"   {var_name:10s}: VIF = {vif:.4f}\", end=\"\")\n",
    "    if vif < 5:\n",
    "        print(\" (No multicollinearity)\")\n",
    "    else:\n",
    "        print(\" (MULTICOLLINEARITY WARNING)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF ASSUMPTIONS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Normality (Jarque-Bera):      {'PASS' if jb_pvalue > 0.05 else 'FAIL'}\")\n",
    "print(f\"Homoscedasticity (BP test):   {'PASS' if bp_pvalue > 0.05 else 'FAIL'}\")\n",
    "print(f\"Independence (Durbin-Watson): {'PASS' if 1.5 < dw_statistic < 2.5 else 'WARNING'}\")\n",
    "print(f\"Multicollinearity (VIF):      {'PASS' if all(vif < 5 for _, vif in vif_values) else 'WARNING'}\")\n",
    "\n",
    "print(\"\\nNote: Even if all tests pass, robust standard errors are still\")\n",
    "print(\"recommended for financial time-series data. Financial returns exhibit\")\n",
    "print(\"time-varying volatility (conditional heteroscedasticity) that the\")\n",
    "print(\"Breusch-Pagan test may not detect. This motivates the next section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab360fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualize: Assumption test dashboard\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# --- Panel 1: p-value summary for normality & homoscedasticity tests ---\n",
    "test_names = ['Jarque-Bera\\n(Normality)', 'Breusch-Pagan\\n(Homoscedasticity)']\n",
    "p_values = [jb_pvalue, bp_pvalue]\n",
    "bar_colors = ['#2ecc71' if p > 0.05 else '#e74c3c' for p in p_values]\n",
    "\n",
    "ax = axes[0]\n",
    "bars = ax.bar(test_names, p_values, color=bar_colors, edgecolor='white', width=0.5)\n",
    "ax.axhline(y=0.05, color='black', linestyle='--', linewidth=1.5, label='α = 0.05')\n",
    "for bar, pv in zip(bars, p_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'p = {pv:.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.set_ylabel('p-value')\n",
    "ax.set_title('Hypothesis Test p-Values', fontweight='bold', fontsize=12)\n",
    "ax.set_ylim(0, max(p_values) * 1.3 + 0.05)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# --- Panel 2: Durbin-Watson gauge ---\n",
    "ax2 = axes[1]\n",
    "dw_range = np.linspace(0, 4, 300)\n",
    "# Color gradient: red at 0 and 4, green at 2\n",
    "dw_colors_map = plt.cm.RdYlGn(1 - np.abs(dw_range - 2) / 2)\n",
    "for i in range(len(dw_range) - 1):\n",
    "    ax2.barh(0, dw_range[i+1] - dw_range[i], left=dw_range[i], height=0.6,\n",
    "             color=dw_colors_map[i])\n",
    "ax2.axvline(x=dw_statistic, color='black', linewidth=3, label=f'DW = {dw_statistic:.3f}')\n",
    "ax2.axvline(x=2.0, color='gray', linewidth=1, linestyle=':', alpha=0.7)\n",
    "ax2.set_xlim(0, 4)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_xlabel('Durbin-Watson Statistic')\n",
    "ax2.set_title('Autocorrelation: Durbin-Watson', fontweight='bold', fontsize=12)\n",
    "ax2.legend(fontsize=10, loc='upper left')\n",
    "ax2.text(0.3, -0.4, 'Positive\\nautocorrelation', ha='center', fontsize=8, color='#c0392b')\n",
    "ax2.text(2.0, -0.4, 'No\\nautocorrelation', ha='center', fontsize=8, color='#27ae60')\n",
    "ax2.text(3.7, -0.4, 'Negative\\nautocorrelation', ha='center', fontsize=8, color='#c0392b')\n",
    "\n",
    "# --- Panel 3: VIF bar chart ---\n",
    "ax3 = axes[2]\n",
    "vif_names = [name for name, _ in vif_values]\n",
    "vif_vals = [vif for _, vif in vif_values]\n",
    "vif_colors = ['#2ecc71' if v < 5 else '#e74c3c' for v in vif_vals]\n",
    "bars3 = ax3.bar(vif_names, vif_vals, color=vif_colors, edgecolor='white', width=0.5)\n",
    "ax3.axhline(y=5, color='black', linestyle='--', linewidth=1.5, label='VIF = 5 threshold')\n",
    "for bar, v in zip(bars3, vif_vals):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "            f'{v:.2f}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax3.set_ylabel('VIF')\n",
    "ax3.set_title('Multicollinearity: VIF', fontweight='bold', fontsize=12)\n",
    "ax3.set_ylim(0, max(max(vif_vals) * 1.3, 6))\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle('Regression Assumption Diagnostics Dashboard', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdb791",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Robust Standard Errors — The Standard in Empirical Finance\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "The diagnostic tests above may or may not have flagged violations. But here's the key insight: **in financial time-series, we almost always use robust standard errors regardless of what the tests say.**\n",
    "\n",
    "Why? Because:\n",
    "- Financial returns exhibit **stochastic volatility** — variance changes over time. This is heteroscedasticity that simple tests may miss.\n",
    "- Monthly returns can show **mild autocorrelation**, especially around regime changes.\n",
    "- Classic OLS standard errors **assume** constant variance and no autocorrelation. If those assumptions fail, your coefficient estimates ($\\hat{\\beta}$) are still unbiased, but your **standard errors, t-statistics, p-values, and confidence intervals are wrong**.\n",
    "\n",
    "> **Full derivation and intuition:** The sandwich formula (HC0), its small-sample variants (HC1–HC3), and the Newey-West extension are derived step-by-step — with hand calculations on a 5-observation dataset — in the [Statistical Foundations](01_Statistical_Foundations.ipynb) (Sections 6–9). The Monte Carlo evidence there demonstrates that classic OLS over-rejects under $H_0$ while Newey-West maintains the correct 5 % size.\n",
    "\n",
    "### An Intuitive Picture: Why Autocorrelation Inflates False Positives\n",
    "\n",
    "Imagine 100 monthly residuals. If each were truly independent, you'd have 100 independent pieces of information. But with **positive autocorrelation**, consecutive residuals are partially redundant: your 100 observations effectively contain **fewer than 100 independent pieces**. Classic OLS doesn't know this — it treats all 100 as fully independent, producing standard errors that are **too small**, t-statistics that are **too large**, and p-values that are **too optimistic**.\n",
    "\n",
    "> This is exactly what Section 5 of the [Time-Series Foundations](02_Time_Series_Foundations.ipynb) notebook showed empirically: autocorrelation in returns decays fast enough for the mixing CLT to work, but is still present enough to distort classic standard errors.\n",
    "\n",
    "### The Solution: Let the Data Determine the Variance\n",
    "\n",
    "Instead of assuming a rigid error structure, robust methods let the residuals themselves determine the variance estimate:\n",
    "\n",
    "| Method | Corrects for | When to use |\n",
    "|--------|-------------|-------------|\n",
    "| **HC1 / HC3** | Heteroscedasticity only | Cross-sectional data |\n",
    "| **Newey-West HAC** | Heteroscedasticity **+** autocorrelation | **Time-series data** (our case) |\n",
    "\n",
    "**For Fama-French regressions: always use Newey-West.** The code below compares all four methods side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effects of heteroscedasticity and autocorrelation on standard errors\n",
    "# and compare different robust methods\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARING STANDARD ERROR ESTIMATES: CLASSIC VS. ROBUST METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fit the model again (same as before)\n",
    "y = df['Stock_Excess_Return'].values\n",
    "X_factors = df[['Mkt_RF', 'SMB', 'HML']].values\n",
    "X_with_const = sm.add_constant(X_factors)\n",
    "\n",
    "model = sm.OLS(y, X_with_const)\n",
    "results = model.fit()\n",
    "\n",
    "# Get classic standard errors\n",
    "print(\"\\n1. CLASSIC OLS STANDARD ERRORS (assumes homoscedastic, independent errors)\")\n",
    "print(\"-\" * 80)\n",
    "classic_summary = results.summary()\n",
    "print(classic_summary.tables[1])\n",
    "\n",
    "classic_se = np.asarray(results.bse)\n",
    "\n",
    "# HC1 (Heteroscedasticity Consistent)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. HC1 STANDARD ERRORS (corrects for heteroscedasticity only)\")\n",
    "print(\"-\" * 80)\n",
    "results_hc1 = results.get_robustcov_results(cov_type='HC1')\n",
    "print(results_hc1.summary().tables[1])\n",
    "\n",
    "hc1_se = np.asarray(results_hc1.bse)\n",
    "\n",
    "# HC3 Huber-White sandwich estimator\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. HC3 STANDARD ERRORS (alternative HC correction)\")\n",
    "print(\"-\" * 80)\n",
    "results_hc3 = results.get_robustcov_results(cov_type='HC3')\n",
    "print(results_hc3.summary().tables[1])\n",
    "\n",
    "hc3_se = np.asarray(results_hc3.bse)\n",
    "\n",
    "# Newey-West Standard Errors (optimal lag selection)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. NEWEY-WEST STANDARD ERRORS (corrects for heteroscedasticity + autocorrelation)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate appropriate lag length\n",
    "n_obs = len(y)\n",
    "lag_length = int(np.floor(4 * (n_obs / 100) ** (2/9)))\n",
    "print(f\"Sample size: {n_obs} observations\")\n",
    "print(f\"Optimal lag length: {lag_length} (using Andrews formula)\")\n",
    "\n",
    "results_nw = results.get_robustcov_results(cov_type='HAC', maxlags=lag_length)\n",
    "print(results_nw.summary().tables[1])\n",
    "\n",
    "nw_se = np.asarray(results_nw.bse)\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED COMPARISON OF STANDARD ERRORS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "var_names = ['Intercept (a)', 'Mkt_RF (B_MKT)', 'SMB (B_SMB)', 'HML (B_HML)']\n",
    "comparison_data = {\n",
    "    'Variable': var_names,\n",
    "    'Classic SE': classic_se,\n",
    "    'HC1 SE': hc1_se,\n",
    "    'HC3 SE': hc3_se,\n",
    "    'Newey-West SE': nw_se,\n",
    "    'HC1 % Change': (hc1_se - classic_se) / classic_se * 100,\n",
    "    'NW % Change': (nw_se - classic_se) / classic_se * 100\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETING THE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "% Change Interpretation:\n",
    "- Positive: Robust method gives LARGER standard errors (more conservative)\n",
    "- Negative: Robust method gives SMALLER standard errors (tighter)\n",
    "- |Change| > 10%: Notable difference, using classic SE could mislead\n",
    "\n",
    "In this example:\n",
    "- If Newey-West SEs are much larger than classic, the data likely exhibits\n",
    "  heteroscedasticity and/or autocorrelation\n",
    "- Classic SEs would make confidence intervals too narrow\n",
    "- Classic t-statistics would be too large in absolute value\n",
    "- Classic p-values would be too small (Type I error: false positives)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023243a",
   "metadata": {},
   "source": [
    "### Quick-Reference: The Sandwich Formula\n",
    "\n",
    "All robust standard errors share the same **sandwich** structure:\n",
    "\n",
    "$$\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) = \\underbrace{(\\mathbf{X}^T \\mathbf{X})^{-1}}_{\\text{bread}} \\;\\; \\underbrace{\\mathbf{M}}_{\\text{meat}} \\;\\; \\underbrace{(\\mathbf{X}^T \\mathbf{X})^{-1}}_{\\text{bread}}$$\n",
    "\n",
    "What differs between methods is the **meat** $\\mathbf{M}$:\n",
    "\n",
    "| Method | Meat | Key feature |\n",
    "|--------|------|-------------|\n",
    "| HC0 | $\\mathbf{X}^T \\text{diag}(\\hat\\epsilon_i^2) \\mathbf{X}$ | Uses squared residuals; no autocorrelation correction |\n",
    "| Newey-West | $\\sum_{j=0}^{L} w_j \\hat\\Gamma_j$ | Adds Bartlett-weighted cross-lag terms up to lag $L$ |\n",
    "\n",
    "**Lag selection** (critical for Newey-West): $L = \\lfloor 4(n/100)^{2/9}\\rfloor$ — the Andrews formula. Too few lags misses correlation; too many introduces noise.\n",
    "\n",
    "> For the complete hand calculation of each formula — HC0, HC1, HC2, HC3, and Newey-West with Bartlett weights — see the [Statistical Foundations](01_Statistical_Foundations.ipynb), Sections 6–7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5176c",
   "metadata": {},
   "source": [
    "### Decision Guide — Which Standard Errors?\n",
    "\n",
    "```\n",
    "Is this TIME-SERIES data?\n",
    "  ├─ NO  (cross-sectional)  →  HC1 or HC3\n",
    "  └─ YES (monthly returns, etc.)  →  Newey-West HAC  ✓\n",
    "```\n",
    "\n",
    "**For the Fama-French model:** Our data is monthly returns (time-series), so **Newey-West is the correct choice**. Many finance journals now require robust standard errors. Let's see how much the choice matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec887557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the impact on inference: how p-values change\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPACT ON HYPOTHESIS TESTING: HOW P-VALUES CHANGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "coefficients = np.asarray(results.params)\n",
    "\n",
    "# Classic method\n",
    "classic_pvalues = np.asarray(results.pvalues)\n",
    "\n",
    "# Robust methods\n",
    "hc1_tstats = coefficients / hc1_se\n",
    "hc1_pvalues = 2 * (1 - stats.t.cdf(np.abs(hc1_tstats), n_obs - 4))\n",
    "\n",
    "nw_tstats = coefficients / nw_se\n",
    "nw_pvalues = 2 * (1 - stats.t.cdf(np.abs(nw_tstats), n_obs - 4))\n",
    "\n",
    "print(\"\\nP-value Comparison (Two-tailed test, a = 0.05)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "pvalue_data = {\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Classic p-val': classic_pvalues,\n",
    "    'HC1 p-val': hc1_pvalues,\n",
    "    'Newey-West p-val': nw_pvalues,\n",
    "    'Significance Change?': []\n",
    "}\n",
    "\n",
    "for i in range(len(var_names)):\n",
    "    classic_sig = \"Yes\" if classic_pvalues[i] < 0.05 else \"No\"\n",
    "    nw_sig = \"Yes\" if nw_pvalues[i] < 0.05 else \"No\"\n",
    "    change = \"YES!\" if classic_sig != nw_sig else \"\"\n",
    "    pvalue_data['Significance Change?'].append(change)\n",
    "\n",
    "pvalue_df = pd.DataFrame(pvalue_data)\n",
    "print(pvalue_df.to_string(index=False))\n",
    "\n",
    "print(\"\"\"\n",
    "CRITICAL: If a variable is significant under classic but NOT under robust SEs\n",
    "(or vice versa), using the classic method could lead to wrong conclusions!\n",
    "\"\"\")\n",
    "\n",
    "# Create visualization comparing confidence intervals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left plot: Coefficient estimates with different confidence intervals\n",
    "ax = axes[0]\n",
    "y_pos = np.arange(len(var_names))\n",
    "\n",
    "# Classic 95% CI\n",
    "classic_ci_lower = coefficients - 1.96 * classic_se\n",
    "classic_ci_upper = coefficients + 1.96 * classic_se\n",
    "\n",
    "# Newey-West 95% CI\n",
    "nw_ci_lower = coefficients - 1.96 * nw_se\n",
    "nw_ci_upper = coefficients + 1.96 * nw_se\n",
    "\n",
    "ax.scatter(coefficients, y_pos, s=100, color='black', zorder=3, label='Point Estimate')\n",
    "ax.hlines(y_pos, classic_ci_lower, classic_ci_upper, colors='blue', linewidth=2, label='Classic 95% CI')\n",
    "ax.hlines(y_pos, nw_ci_lower, nw_ci_upper, colors='red', linewidth=3, alpha=0.6, label='Newey-West 95% CI')\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(var_names)\n",
    "ax.set_xlabel('Coefficient Value')\n",
    "ax.set_title('95% Confidence Intervals: Classic vs. Newey-West\\n(Wider intervals = more conservative)', \n",
    "             fontweight='bold', fontsize=12)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Right plot: Standard error comparison\n",
    "ax = axes[1]\n",
    "x_pos = np.arange(len(var_names))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x_pos - width, classic_se, width, label='Classic', alpha=0.8)\n",
    "ax.bar(x_pos, hc1_se, width, label='HC1', alpha=0.8)\n",
    "ax.bar(x_pos + width, nw_se, width, label='Newey-West', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Variables')\n",
    "ax.set_ylabel('Standard Error')\n",
    "ax.set_title('Standard Error Comparison\\n(Higher = More Uncertainty)', fontweight='bold', fontsize=12)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(var_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('robust_se_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved: Confidence interval and standard error comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d490366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the RECOMMENDED approach: Newey-West for Fama-French analysis\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RECOMMENDED APPROACH: FAMA-FRENCH 3-FACTOR WITH NEWEY-WEST SEs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nOur data: Monthly ETF returns (time-series)\")\n",
    "print(f\"Sample: {df.index[0].strftime('%B %Y')} to {df.index[-1].strftime('%B %Y')}\")\n",
    "print(f\"Number of observations: {len(df)}\")\n",
    "\n",
    "# Fit with Newey-West standard errors\n",
    "lag_length = int(np.floor(4 * (len(df) / 100) ** (2/9)))\n",
    "results_recommended = results.get_robustcov_results(cov_type='HAC', maxlags=lag_length)\n",
    "\n",
    "print(f\"\\nNewey-West lag length (Andrews formula): {lag_length}\")\n",
    "print(\"\\nFull Results with Newey-West HAC Standard Errors:\")\n",
    "print(\"=\"*80)\n",
    "print(results_recommended.summary())\n",
    "\n",
    "print(f\"\"\"\n",
    "WHY THE NEWEY-WEST APPROACH IS BETTER\n",
    "{'='*80}\n",
    "\n",
    "Comparison of Standard Errors:\n",
    "                  Classic SE    Newey-West SE    Difference\n",
    "Alpha:            {results.bse[0]:11.6f}    {results_recommended.bse[0]:15.6f}    {((results_recommended.bse[0]-results.bse[0])/results.bse[0]*100):+6.1f}%\n",
    "Beta_MKT:         {results.bse[1]:11.6f}    {results_recommended.bse[1]:15.6f}    {((results_recommended.bse[1]-results.bse[1])/results.bse[1]*100):+6.1f}%\n",
    "Beta_SMB:         {results.bse[2]:11.6f}    {results_recommended.bse[2]:15.6f}    {((results_recommended.bse[2]-results.bse[2])/results.bse[2]*100):+6.1f}%\n",
    "Beta_HML:         {results.bse[3]:11.6f}    {results_recommended.bse[3]:15.6f}    {((results_recommended.bse[3]-results.bse[3])/results.bse[3]*100):+6.1f}%\n",
    "\n",
    "Key Benefits of Newey-West:\n",
    "\n",
    "1. ACCOUNTS FOR AUTOCORRELATION\n",
    "   - Monthly returns are not independent\n",
    "   - Using classic SEs underestimates true uncertainty\n",
    "\n",
    "2. ACCOUNTS FOR HETEROSCEDASTICITY  \n",
    "   - Asset volatility varies over time\n",
    "   - Classic SEs assume constant variance\n",
    "   \n",
    "3. MAKES INFERENCE MORE CONSERVATIVE\n",
    "   - Wider confidence intervals = more honest about uncertainty\n",
    "   - Lower risk of Type I error (false positives)\n",
    "\n",
    "4. STANDARD IN FINANCE RESEARCH\n",
    "   - Published papers use robust SEs\n",
    "   - Reviewers expect robust standard errors\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Now that we have reliable standard errors, we can properly\")\n",
    "print(\"interpret the coefficients and their significance (next section).\")\n",
    "print()\n",
    "print(\"Quick guide — how to read the Newey-West output above:\")\n",
    "print(\"  • |t| > 2  and  p < 0.05  →  coefficient is statistically significant\")\n",
    "print(\"  • β_MKT > 1 → more volatile than the market; < 1 → more defensive\")\n",
    "print(\"  • β_SMB > 0 → tilts toward small caps; < 0 → tilts toward large caps\")\n",
    "print(\"  • β_HML > 0 → tilts toward value; < 0 → tilts toward growth\")\n",
    "print(\"  • α ≈ 0 and not significant → no abnormal return beyond factor exposure\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe130c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Interpreting the Results — t-Statistics, p-Values, and Confidence Intervals\n",
    "\n",
    "Now that we have **reliable standard errors** (Newey-West), we can properly interpret each coefficient and its statistical significance.\n",
    "\n",
    "> **Why can we use the normal distribution for p-values here?** The CLT guarantees that OLS coefficients are approximately normally distributed for large $n$, **regardless of the error distribution**. Combined with Newey-West and Slutsky's theorem, the t-statistic converges to $\\mathcal{N}(0,1)$. For the full five-step argument, see the [Statistical Foundations Tutorial](01_Statistical_Foundations.ipynb), Section 9.\n",
    "\n",
    "### Quick Refresher (detailed derivations: [Statistical Foundations](01_Statistical_Foundations.ipynb), Section 4)\n",
    "\n",
    "| Statistic | Formula | Rule of thumb |\n",
    "|-----------|---------|---------------|\n",
    "| **t-statistic** | $t = \\hat{\\beta}_j \\,/\\, \\text{SE}(\\hat{\\beta}_j)$ | $|t| > 2$ → significant |\n",
    "| **p-value** | Prob of result this extreme under $H_0:\\beta_j=0$ | $p < 0.05$ → reject $H_0$ |\n",
    "| **95 % CI** | $\\hat{\\beta}_j \\pm 1.96 \\times \\text{SE}(\\hat{\\beta}_j)$ | CI excludes 0 → significant |\n",
    "\n",
    "**Statistical vs economic significance:** A tiny but precisely estimated effect can be statistically significant (small $p$) without being economically meaningful. For example, $\\hat\\alpha = 0.0002$ (0.02 %/month ≈ 0.24 %/year) may matter statistically but is smaller than most transaction costs. Always ask: *Is the magnitude large enough to matter in practice?*\n",
    "\n",
    "### Interpreting the Fama-French Coefficients\n",
    "\n",
    "| Coefficient | What it measures | Positive means | Negative means |\n",
    "|-------------|-----------------|----------------|----------------|\n",
    "| $\\alpha$ (Jensen's Alpha) | Abnormal return beyond factor exposure | Outperformance | Underperformance |\n",
    "| $\\beta_{MKT}$ | Market sensitivity | Aggressive ($>1$) | Defensive ($<1$) |\n",
    "| $\\beta_{SMB}$ | Size tilt | Small-cap tilt | Large-cap tilt |\n",
    "| $\\beta_{HML}$ | Value tilt | Value tilt | Growth tilt |\n",
    "\n",
    "### Let's Now Interpret Our Fama-French Results Using Newey-West Standard Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e792892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret each coefficient using Newey-West robust standard errors\n",
    "print(\"=\"*70)\n",
    "print(\"DETAILED INTERPRETATION OF COEFFICIENTS (NEWEY-WEST SEs)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "coef_nw = results_recommended.params\n",
    "se_nw = results_recommended.bse\n",
    "pval_nw = results_recommended.pvalues\n",
    "ci_nw = results_recommended.conf_int()\n",
    "\n",
    "# Extract each coefficient\n",
    "alpha = coef_nw[0]\n",
    "beta_mkt = coef_nw[1]\n",
    "beta_smb = coef_nw[2]\n",
    "beta_hml = coef_nw[3]\n",
    "\n",
    "print(f\"\\n1. ALPHA (Jensen's Alpha)\")\n",
    "print(f\"   Estimate: {alpha:.6f} ({alpha*100:.4f}% per month)\")\n",
    "print(f\"   Newey-West Std Error: {se_nw[0]:.6f}\")\n",
    "print(f\"   t-statistic: {alpha/se_nw[0]:.4f}\")\n",
    "print(f\"   p-value: {pval_nw[0]:.4f}\")\n",
    "print(f\"   95% CI: [{ci_nw[0, 0]:.6f}, {ci_nw[0, 1]:.6f}]\")\n",
    "print(f\"   Interpretation: This is Jensen's alpha. It represents the abnormal\")\n",
    "print(f\"   monthly return not explained by the three factors. A value of {alpha:.4f}\")\n",
    "print(f\"   ({alpha*100:.2f}% per month) means this ETF earns an excess return of\")\n",
    "print(f\"   {alpha*100:.2f}% per month beyond what the model predicts.\")\n",
    "if pval_nw[0] < 0.05:\n",
    "    print(f\"   STATISTICALLY SIGNIFICANT (p < 0.05): The ETF appears to have abnormal alpha.\")\n",
    "else:\n",
    "    print(f\"   NOT statistically significant (p >= 0.05): Cannot distinguish alpha from zero.\")\n",
    "\n",
    "print(f\"\\n2. BETA_MKT (Market Beta)\")\n",
    "print(f\"   Estimate: {beta_mkt:.6f}\")\n",
    "print(f\"   Newey-West Std Error: {se_nw[1]:.6f}\")\n",
    "print(f\"   t-statistic: {beta_mkt/se_nw[1]:.4f}\")\n",
    "print(f\"   p-value: {pval_nw[1]:.4f}\")\n",
    "print(f\"   95% CI: [{ci_nw[1, 0]:.6f}, {ci_nw[1, 1]:.6f}]\")\n",
    "print(f\"   Interpretation: For every 1% increase in the market excess return,\")\n",
    "print(f\"   the ETF's excess return changes by {beta_mkt:.4f}% (holding other\")\n",
    "print(f\"   factors constant). A value > 1 means the ETF is MORE volatile than\")\n",
    "print(f\"   the market; < 1 means LESS volatile (more defensive).\")\n",
    "if pval_nw[1] < 0.05:\n",
    "    print(f\"   STATISTICALLY SIGNIFICANT: Market exposure is real.\")\n",
    "else:\n",
    "    print(f\"   NOT statistically significant.\")\n",
    "\n",
    "print(f\"\\n3. BETA_SMB (Size Factor)\")\n",
    "print(f\"   Estimate: {beta_smb:.6f}\")\n",
    "print(f\"   Newey-West Std Error: {se_nw[2]:.6f}\")\n",
    "print(f\"   t-statistic: {beta_smb/se_nw[2]:.4f}\")\n",
    "print(f\"   p-value: {pval_nw[2]:.4f}\")\n",
    "print(f\"   95% CI: [{ci_nw[2, 0]:.6f}, {ci_nw[2, 1]:.6f}]\")\n",
    "print(f\"   Interpretation: For every 1% increase in the SMB factor (small\")\n",
    "print(f\"   relative to big), the ETF's excess return changes by {beta_smb:.4f}%.\")\n",
    "print(f\"   A positive value means the ETF tilts toward SMALL-cap exposure.\")\n",
    "print(f\"   A negative value would mean it tilts toward LARGE-cap exposure.\")\n",
    "if pval_nw[2] < 0.05:\n",
    "    print(f\"   STATISTICALLY SIGNIFICANT: Real size effect exposure.\")\n",
    "else:\n",
    "    print(f\"   NOT statistically significant: Size effect not clearly present.\")\n",
    "\n",
    "print(f\"\\n4. BETA_HML (Value Factor)\")\n",
    "print(f\"   Estimate: {beta_hml:.6f}\")\n",
    "print(f\"   Newey-West Std Error: {se_nw[3]:.6f}\")\n",
    "print(f\"   t-statistic: {beta_hml/se_nw[3]:.4f}\")\n",
    "print(f\"   p-value: {pval_nw[3]:.4f}\")\n",
    "print(f\"   95% CI: [{ci_nw[3, 0]:.6f}, {ci_nw[3, 1]:.6f}]\")\n",
    "print(f\"   Interpretation: For every 1% increase in the HML factor (value\")\n",
    "print(f\"   relative to growth), the ETF's excess return changes by {beta_hml:.4f}%.\")\n",
    "print(f\"   A positive value means the ETF tilts toward VALUE exposure.\")\n",
    "print(f\"   A negative value would mean it tilts toward GROWTH exposure.\")\n",
    "if pval_nw[3] < 0.05:\n",
    "    print(f\"   STATISTICALLY SIGNIFICANT: Real value effect exposure.\")\n",
    "else:\n",
    "    print(f\"   NOT statistically significant: Value effect not clearly present.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY TAKEAWAY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "All the standard errors, t-statistics, p-values, and confidence intervals\n",
    "above use Newey-West HAC estimation. This means:\n",
    "\n",
    "- They account for heteroscedasticity (time-varying volatility)\n",
    "- They account for autocorrelation (serial dependence in returns)\n",
    "- The conclusions are MORE RELIABLE than using classic OLS standard errors\n",
    "\n",
    "When reporting Fama-French results:\n",
    "  - Always use Newey-West HAC standard errors for time-series data\n",
    "  - Report the lag length used (here: Andrews formula)\n",
    "  - Discuss economic significance separately from statistical significance\n",
    "  - Compare with classic SEs as a robustness check\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bef4a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Complete Worked Example - Using the Model to Make Predictions\n",
    "\n",
    "Now let's use our fitted model to make a prediction and see how the model works in practice.\n",
    "\n",
    "### The Prediction Formula\n",
    "\n",
    "For any month with known factor values, we can predict the ETF's excess return using:\n",
    "\n",
    "$$\\hat{R}_i = \\hat{\\alpha} + \\hat{\\beta}_{MKT} \\cdot (R_m - R_f) + \\hat{\\beta}_{SMB} \\cdot SMB + \\hat{\\beta}_{HML} \\cdot HML$$\n",
    "\n",
    "Let's make a prediction for a specific month and compare it to the actual return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d251176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"WORKED EXAMPLE: PREDICTING ETF RETURN FOR A SPECIFIC MONTH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Let's use the first month in our data\n",
    "month_idx = 50  # Choose a month in the middle of data\n",
    "date = df.index[month_idx]\n",
    "\n",
    "# Get the actual values for this month\n",
    "mkt_rf_actual = df['Mkt_RF'].iloc[month_idx]\n",
    "smb_actual = df['SMB'].iloc[month_idx]\n",
    "hml_actual = df['HML'].iloc[month_idx]\n",
    "stock_return_actual = df['Stock_Excess_Return'].iloc[month_idx]\n",
    "\n",
    "print(f\"\\nDate: {date.strftime('%B %Y')}\")\n",
    "print(f\"\\nFactor values for this month:\")\n",
    "print(f\"  Market Risk Premium (Mkt_RF): {mkt_rf_actual:.4f} ({mkt_rf_actual*100:.2f}%)\")\n",
    "print(f\"  SMB Factor:                    {smb_actual:.4f} ({smb_actual*100:.2f}%)\")\n",
    "print(f\"  HML Factor:                    {hml_actual:.4f} ({hml_actual*100:.2f}%)\")\n",
    "\n",
    "# Get our estimated coefficients\n",
    "alpha_hat = results.params[0]\n",
    "beta_mkt_hat = results.params[1]\n",
    "beta_smb_hat = results.params[2]\n",
    "beta_hml_hat = results.params[3]\n",
    "\n",
    "print(f\"\\nOur estimated model coefficients:\")\n",
    "print(f\"  α̂ (Alpha):     {alpha_hat:.6f}\")\n",
    "print(f\"  β̂_MKT:         {beta_mkt_hat:.6f}\")\n",
    "print(f\"  β̂_SMB:         {beta_smb_hat:.6f}\")\n",
    "print(f\"  β̂_HML:         {beta_hml_hat:.6f}\")\n",
    "\n",
    "# Make the prediction manually showing all steps\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION CALCULATION (STEP BY STEP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFormula: R̂_i = α̂ + β̂_MKT × Mkt_RF + β̂_SMB × SMB + β̂_HML × HML\")\n",
    "print(f\"\\nR̂_i = {alpha_hat:.6f}\")\n",
    "print(f\"     + {beta_mkt_hat:.6f} × {mkt_rf_actual:.6f}\")\n",
    "print(f\"     + {beta_smb_hat:.6f} × {smb_actual:.6f}\")\n",
    "print(f\"     + {beta_hml_hat:.6f} × {hml_actual:.6f}\")\n",
    "\n",
    "# Calculate each component\n",
    "component_alpha = alpha_hat\n",
    "component_mkt = beta_mkt_hat * mkt_rf_actual\n",
    "component_smb = beta_smb_hat * smb_actual\n",
    "component_hml = beta_hml_hat * hml_actual\n",
    "\n",
    "print(f\"\\nComponent breakdown:\")\n",
    "print(f\"  α̂:                    {component_alpha:.6f} ({component_alpha*100:.4f}%)\")\n",
    "print(f\"  β̂_MKT × Mkt_RF:       {component_mkt:.6f} ({component_mkt*100:.4f}%)\")\n",
    "print(f\"  β̂_SMB × SMB:          {component_smb:.6f} ({component_smb*100:.4f}%)\")\n",
    "print(f\"  β̂_HML × HML:          {component_hml:.6f} ({component_hml*100:.4f}%)\")\n",
    "\n",
    "# Total prediction\n",
    "stock_return_pred = component_alpha + component_mkt + component_smb + component_hml\n",
    "\n",
    "print(f\"\\nPredicted return (sum): {stock_return_pred:.6f} ({stock_return_pred*100:.4f}%)\")\n",
    "print(f\"Actual return:          {stock_return_actual:.6f} ({stock_return_actual*100:.4f}%)\")\n",
    "\n",
    "# Calculate prediction error (residual)\n",
    "residual = stock_return_actual - stock_return_pred\n",
    "print(f\"\\nPrediction error (residual): {residual:.6f} ({residual*100:.4f}%)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if abs(residual) < 0.01:\n",
    "    print(f\"  The model predicted very accurately for this month!\")\n",
    "elif residual > 0:\n",
    "    print(f\"  The ETF OUTPERFORMED the model's prediction by {abs(residual)*100:.4f}%\")\n",
    "    print(f\"  This positive residual means the ETF did better than expected\")\n",
    "    print(f\"  given its exposure to the three factors.\")\n",
    "else:\n",
    "    print(f\"  The ETF UNDERPERFORMED the model's prediction by {abs(residual)*100:.4f}%\")\n",
    "    print(f\"  This negative residual means the ETF did worse than expected\")\n",
    "    print(f\"  given its exposure to the three factors.\")\n",
    "\n",
    "# Verify using statsmodels\n",
    "pred_check = results.fittedvalues[month_idx]\n",
    "pred_error_check = results.resid[month_idx]\n",
    "print(f\"\\nVerification with statsmodels:\")\n",
    "print(f\"  Fitted value from statsmodels:  {pred_check:.6f}\")\n",
    "print(f\"  Residual from statsmodels:      {pred_error_check:.6f}\")\n",
    "print(f\"  Match? {np.isclose(stock_return_pred, pred_check) and np.isclose(residual, pred_error_check)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71414d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual returns\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "axes[0].plot(df.index, df['Stock_Excess_Return']*100, label='Actual Returns', \n",
    "             linewidth=2, alpha=0.7, color='darkblue')\n",
    "axes[0].plot(df.index, results.fittedvalues*100, label='Model Predictions', \n",
    "             linewidth=2, alpha=0.7, color='darkred', linestyle='--')\n",
    "axes[0].set_ylabel('Return (%)', fontsize=11)\n",
    "axes[0].set_title('Actual vs Model-Predicted ETF Returns', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals (prediction errors)\n",
    "axes[1].bar(df.index, results.resid*100, alpha=0.7, color='steelblue', width=20)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_ylabel('Residual (%)', fontsize=11)\n",
    "axes[1].set_xlabel('Date', fontsize=11)\n",
    "axes[1].set_title('Prediction Errors (Residuals) Over Time', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fama_french_predictions.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction visualization created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb48a7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 12: Complete Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "In this comprehensive tutorial, we covered:\n",
    "\n",
    "1. **The Fama-French Model Concept** (Sections 1-2): A framework to explain stock returns using three factors — market risk premium, size effect (SMB), and value effect (HML)\n",
    "\n",
    "2. **Linear Regression Mathematics** (Sections 4-5): Setting up the design matrix, the normal equations, and the OLS formula to obtain coefficient estimates\n",
    "\n",
    "3. **Model Evaluation** (Section 7): $R^2$ and adjusted $R^2$ to measure fit, residual standard error, and F-statistics to test overall model significance\n",
    "\n",
    "4. **Diagnostic Testing** (Section 8): Checking regression assumptions through visual diagnostics and formal tests (Jarque-Bera, Breusch-Pagan, Durbin-Watson, VIF)\n",
    "\n",
    "5. **Robust Inference** (Section 9): Why classic standard errors can be misleading for financial time-series, and how Newey-West HAC standard errors provide reliable inference\n",
    "\n",
    "6. **Statistical Interpretation** (Section 10): How to properly interpret t-statistics, p-values, and confidence intervals using robust standard errors — the approach used in published finance research\n",
    "\n",
    "7. **Practical Application** (Section 11): Making predictions using the fitted model and understanding the decomposition of returns\n",
    "\n",
    "### Key Formulas Summary\n",
    "\n",
    "| Concept | Formula |\n",
    "|---------|---------|\n",
    "| **Model Equation** | $R_i - R_f = \\alpha + \\beta_{MKT}(R_m - R_f) + \\beta_{SMB} \\cdot SMB + \\beta_{HML} \\cdot HML + \\epsilon$ |\n",
    "| **OLS Estimator** | $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$ |\n",
    "| **R-squared** | $R^2 = 1 - \\frac{RSS}{TSS}$ |\n",
    "| **Adjusted R-squared** | $\\bar{R}^2 = 1 - \\frac{(1-R^2)(n-1)}{n-k}$ |\n",
    "| **t-statistic** | $t = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)}$ |\n",
    "| **Newey-West Var** | $\\hat{\\text{Var}}_{NW}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T \\mathbf{X})^{-1} \\left[ \\sum_{j=0}^{M} w_j \\mathbf{X}^T \\boldsymbol{\\Omega}_j \\mathbf{X} \\right] (\\mathbf{X}^T \\mathbf{X})^{-1}$ |\n",
    "\n",
    "### Interpreting Your Results\n",
    "\n",
    "When you fit a Fama-French model to real data:\n",
    "\n",
    "1. **Check the R-squared**: A higher $R^2$ means the three factors explain more of the asset's returns\n",
    "   - $R^2 < 0.3$: Factors don't explain much variation\n",
    "   - $0.3 < R^2 < 0.6$: Factors explain moderate variation\n",
    "   - $R^2 > 0.6$: Factors explain substantial variation\n",
    "\n",
    "2. **Use robust standard errors**: Always use Newey-West HAC for time-series regressions. Report the lag length and note the method.\n",
    "\n",
    "3. **Examine the Alpha**: \n",
    "   - Positive, significant alpha: Asset outperforms (alpha-generating ability)\n",
    "   - Negative, significant alpha: Asset underperforms\n",
    "   - Insignificant alpha: Can't distinguish from luck\n",
    "\n",
    "4. **Look at the Betas**:\n",
    "   - $\\beta_{MKT}$: Is the asset aggressive ($>1$) or defensive ($<1$)?\n",
    "   - $\\beta_{SMB}$: Does it tilt toward small caps ($>0$) or large caps ($<0$)?\n",
    "   - $\\beta_{HML}$: Does it tilt toward value ($>0$) or growth ($<0$)?\n",
    "\n",
    "5. **Distinguish statistical from economic significance**: A tiny but precisely estimated effect might be statistically significant without being economically meaningful.\n",
    "\n",
    "### Common Extensions\n",
    "\n",
    "Once you understand this framework, you can extend it:\n",
    "\n",
    "1. **Add more factors**: The Fama-French 5-factor model adds profitability and investment factors\n",
    "2. **Time-varying betas**: Allow coefficients to change over time using rolling windows\n",
    "3. **Alternative specifications**: Use different factor definitions or construct your own factors\n",
    "4. **Cross-sectional analysis**: Analyze many stocks simultaneously\n",
    "5. **Robustness checks**: Use different time periods, rolling windows, or subsamples\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "The Fama-French model is not just a statistical exercise — it's a tool for understanding investment risk and return. By decomposing returns into factor exposures, investors can:\n",
    "- Understand what they're really betting on\n",
    "- Manage risk more intentionally\n",
    "- Evaluate manager performance fairly\n",
    "- Construct portfolios aligned with their views\n",
    "\n",
    "The regression analysis skills you've learned here extend far beyond finance — they apply to any field where you need to understand relationships in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf609738",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Useful Resources and Reference Code\n",
    "\n",
    "### Getting Real Fama-French Data\n",
    "\n",
    "In this tutorial we downloaded factors directly from Kenneth French's website. Here are alternative approaches:\n",
    "\n",
    "```python\n",
    "# Option 1: Direct download (used in this notebook)\n",
    "import urllib.request, zipfile, tempfile, pandas as pd\n",
    "url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip'\n",
    "\n",
    "# Option 2: Use pandas_datareader library (requires: pip install pandas-datareader)\n",
    "from pandas_datareader import data as web\n",
    "ff_factors = web.DataReader('F-F_Research_Data_Factors', 'famafrench',\n",
    "                            start='2010-01-01', end='2023-12-31')\n",
    "\n",
    "# Option 3: Use yfinance for ETF/stock returns\n",
    "import yfinance as yf\n",
    "etf = yf.download('SPY', start='2010-01-01', end='2023-12-31')\n",
    "```\n",
    "\n",
    "### Alternative Factor Sets\n",
    "\n",
    "Kenneth French also publishes regional and global factor data:\n",
    "- **Global Factors**: `F-F_Research_Data_Factors` -> replace with `Global_3_Factors`\n",
    "- **European Factors**: `Europe_3_Factors`\n",
    "- **Developed Markets**: `Developed_3_Factors`\n",
    "\n",
    "Using region-matched factors (e.g., European factors for European ETFs) can improve model fit for non-US assets.\n",
    "\n",
    "### Quick Reference: Key Statistical Concepts\n",
    "\n",
    "**p-value**: Probability of observing this result by chance if null hypothesis is true\n",
    "- $p < 0.01$: **Highly significant** (strong evidence)\n",
    "- $0.01 < p < 0.05$: **Significant** (moderate evidence)\n",
    "- $0.05 < p < 0.10$: **Marginally significant** (weak evidence)\n",
    "- $p > 0.10$: **Not significant** (insufficient evidence)\n",
    "\n",
    "**Confidence Interval**: Range where we're confident (usually 95%) the true value lies\n",
    "- If CI contains 0: Coefficient might not be significantly different from zero\n",
    "\n",
    "**Standard Error**: Measure of uncertainty in the estimate\n",
    "- Smaller SE = More precise estimate\n",
    "- Larger SE = More uncertainty\n",
    "\n",
    "### Troubleshooting Common Issues\n",
    "\n",
    "| Problem | Likely Cause | Solution |\n",
    "|---------|-------------|----------|\n",
    "| Very low $R^2$ | Missing important factors | Add more relevant variables |\n",
    "| High p-values for all betas | Multicollinearity | Check VIF, remove correlated variables |\n",
    "| Non-normal residuals | Outliers or wrong model | Check for outliers, try robust regression |\n",
    "| Heteroscedasticity | Unequal error variance | Use robust standard errors |\n",
    "| Autocorrelation | Time-series pattern | Use time-series methods, transform data |\n",
    "\n",
    "### Quick Regression Summary Template\n",
    "\n",
    "For any regression analysis, report:\n",
    "1. **The model**: What's the dependent and independent variables?\n",
    "2. **Sample**: How many observations? What time period?\n",
    "3. **Coefficients**: What are the estimates and are they significant?\n",
    "4. **Fit quality**: What's the $R^2$? Do assumptions hold?\n",
    "5. **Economic significance**: Are the effects meaningful in practice?\n",
    "6. **Robustness**: Did you check sensitivity to different specifications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdaf45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REUSABLE FAMA-FRENCH ANALYSIS TEMPLATE\n",
    "# ============================================================================\n",
    "# Use this template as a starting point for analyzing your own stock/portfolio\n",
    "# ============================================================================\n",
    "\n",
    "def fama_french_analysis(stock_excess_returns, mkt_rf, smb, hml, stock_name=\"Stock\", use_robust_se=True):\n",
    "    \"\"\"\n",
    "    Complete Fama-French 3-factor analysis in one function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stock_excess_returns : array-like\n",
    "        Excess returns of the stock/portfolio (length n)\n",
    "    mkt_rf : array-like\n",
    "        Market risk premium (length n)\n",
    "    smb : array-like\n",
    "        SMB factor returns (length n)\n",
    "    hml : array-like\n",
    "        HML factor returns (length n)\n",
    "    stock_name : str\n",
    "        Name of the stock for display purposes\n",
    "    use_robust_se : bool\n",
    "        If True, use Newey-West HAC standard errors (recommended for time-series)\n",
    "        If False, use classic OLS standard errors\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : OLS regression results object (always the same type)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"FAMA-FRENCH 3-FACTOR ANALYSIS: {stock_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Prepare data\n",
    "    y = np.array(stock_excess_returns)\n",
    "    X = np.column_stack([mkt_rf, smb, hml])\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit model\n",
    "    model = sm.OLS(y, X)\n",
    "    r = model.fit()\n",
    "    \n",
    "    # Apply robust standard errors if requested\n",
    "    if use_robust_se:\n",
    "        n_obs = len(y)\n",
    "        lag_length = int(np.floor(4 * (n_obs / 100) ** (2/9)))\n",
    "        r = r.get_robustcov_results(cov_type='HAC', maxlags=lag_length)\n",
    "        se_type = f\"Newey-West HAC (lags={lag_length})\"\n",
    "    else:\n",
    "        se_type = \"Classic OLS\"\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nUsing {se_type} standard errors\\n\")\n",
    "    print(r.summary())\n",
    "    \n",
    "    # Extract key statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY FINDINGS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    alpha, beta_mkt, beta_smb, beta_hml = r.params\n",
    "    p_vals = r.pvalues\n",
    "    \n",
    "    print(f\"\\nalpha:     {alpha:9.6f}  p-value: {p_vals[0]:.4f}  {'***' if p_vals[0] < 0.01 else ('**' if p_vals[0] < 0.05 else '')}\")\n",
    "    print(f\"beta_mkt:  {beta_mkt:9.6f}  p-value: {p_vals[1]:.4f}  {'***' if p_vals[1] < 0.01 else ('**' if p_vals[1] < 0.05 else '')}\")\n",
    "    print(f\"beta_smb:  {beta_smb:9.6f}  p-value: {p_vals[2]:.4f}  {'***' if p_vals[2] < 0.01 else ('**' if p_vals[2] < 0.05 else '')}\")\n",
    "    print(f\"beta_hml:  {beta_hml:9.6f}  p-value: {p_vals[3]:.4f}  {'***' if p_vals[3] < 0.01 else ('**' if p_vals[3] < 0.05 else '')}\")\n",
    "    \n",
    "    print(f\"\\nR-squared: {r.rsquared:.6f}\")\n",
    "    print(f\"Adj R-squared: {r.rsquared_adj:.6f}\")\n",
    "    print(f\"F-statistic: {r.fvalue:.4f} (p < {r.f_pvalue:.2e})\")\n",
    "    \n",
    "    print(\"\\nNote: *** p<0.01, ** p<0.05, * p<0.10\")\n",
    "    \n",
    "    return r\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON OF ALL THREE ETFs\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"# COMPARING FACTOR EXPOSURES: SPY vs. IWN vs. VTV\")\n",
    "print(\"#\"*70)\n",
    "print()\n",
    "print(\"We now run the same Fama-French regression on all three ETFs and\")\n",
    "print(\"compare their factor loadings. This reveals how each investment\")\n",
    "print(\"style maps onto the market, size, and value dimensions.\\n\")\n",
    "\n",
    "all_results = {}\n",
    "for ticker, label in etf_tickers.items():\n",
    "    edf = etf_dataframes[ticker]\n",
    "    res = fama_french_analysis(\n",
    "        edf['Stock_Excess_Return'].values,\n",
    "        edf['Mkt_RF'].values,\n",
    "        edf['SMB'].values,\n",
    "        edf['HML'].values,\n",
    "        stock_name=label,\n",
    "        use_robust_se=True\n",
    "    )\n",
    "    all_results[ticker] = res\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Summary comparison table\n",
    "print(\"=\"*70)\n",
    "print(\"SIDE-BY-SIDE FACTOR LOADING COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "header = f\"{'':20s}  {'SPY':>10s}  {'IWN':>10s}  {'VTV':>10s}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "labels_short = list(etf_tickers.keys())\n",
    "row_names = ['Alpha (a)', 'Beta MKT', 'Beta SMB', 'Beta HML', 'R-squared', 'Adj R-squared']\n",
    "\n",
    "for i, name in enumerate(row_names):\n",
    "    vals = []\n",
    "    for t in labels_short:\n",
    "        r = all_results[t]\n",
    "        if i < 4:\n",
    "            vals.append(f\"{r.params[i]:10.4f}\")\n",
    "        elif i == 4:\n",
    "            vals.append(f\"{r.rsquared:10.4f}\")\n",
    "        else:\n",
    "            vals.append(f\"{r.rsquared_adj:10.4f}\")\n",
    "    print(f\"{name:20s}  {'  '.join(vals)}\")\n",
    "\n",
    "# Significance markers\n",
    "print()\n",
    "for i, name in enumerate(['Alpha', 'Beta MKT', 'Beta SMB', 'Beta HML']):\n",
    "    sigs = []\n",
    "    for t in labels_short:\n",
    "        p = all_results[t].pvalues[i]\n",
    "        star = '***' if p < 0.01 else ('**' if p < 0.05 else ('*' if p < 0.10 else ''))\n",
    "        sigs.append(f\"{star:>10s}\")\n",
    "    print(f\"{'  p-sig ' + name:20s}  {'  '.join(sigs)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "What to look for in the table above:\n",
    "\n",
    "- SPY (S&P 500): Should have beta_MKT very close to 1.0, with low/insignificant\n",
    "  SMB and HML loadings. It IS the market, so the model should explain its\n",
    "  returns almost entirely through the market factor. Expect high R-squared.\n",
    "\n",
    "- IWN (Small Cap Value): Should have POSITIVE beta_SMB (small-cap tilt) AND\n",
    "  POSITIVE beta_HML (value tilt). This ETF explicitly targets small companies\n",
    "  with high book-to-market ratios -- exactly the stocks that drive the SMB\n",
    "  and HML factors.\n",
    "\n",
    "- VTV (Large Cap Value): Should have near-zero or NEGATIVE beta_SMB (large-cap)\n",
    "  and POSITIVE beta_HML (value tilt). This isolates the value effect from\n",
    "  the size effect, showing that HML captures something distinct from SMB.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fecf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE FACTOR LOADINGS ACROSS ALL THREE ETFs\n",
    "# ============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "var_names_bar = ['Beta MKT', 'Beta SMB', 'Beta HML']\n",
    "x_pos = np.arange(len(var_names_bar))\n",
    "width = 0.25\n",
    "colors_bar = {'SPY': 'darkblue', 'IWN': 'darkorange', 'VTV': 'darkgreen'}\n",
    "\n",
    "for i, (ticker, label) in enumerate(etf_tickers.items()):\n",
    "    r = all_results[ticker]\n",
    "    betas = [r.params[1], r.params[2], r.params[3]]  # MKT, SMB, HML\n",
    "    pvals = [r.pvalues[1], r.pvalues[2], r.pvalues[3]]\n",
    "    bars = ax.bar(x_pos + i * width, betas, width, label=label,\n",
    "                  color=colors_bar[ticker], alpha=0.85)\n",
    "    # Add significance stars\n",
    "    for j, bar in enumerate(bars):\n",
    "        p = pvals[j]\n",
    "        star = '***' if p < 0.01 else ('**' if p < 0.05 else ('*' if p < 0.10 else ''))\n",
    "        if star:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                    star, ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xticks(x_pos + width)\n",
    "ax.set_xticklabels(var_names_bar)\n",
    "ax.set_ylabel('Factor Loading (β)')\n",
    "ax.set_title('Fama-French 3-Factor Loadings: SPY vs IWN vs VTV\\n'\n",
    "             '(*** p<0.01, ** p<0.05, * p<0.10 — Newey-West SEs)',\n",
    "             fontweight='bold', fontsize=12)\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('etf_factor_loadings_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "This chart makes the factor exposures immediately visible:\n",
    "- SPY: Dominated by market beta (~1.0), with minimal size or value tilts\n",
    "- IWN: Positive loadings on ALL three factors, especially SMB and HML\n",
    "- VTV: Strong market and value exposure, but low/negative size exposure\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188a661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Test your understanding by working through these exercises. They range from conceptual questions to hands-on coding tasks.\n",
    "\n",
    "### Exercise 1: Predicting Factor Loadings (Conceptual)\n",
    "\n",
    "Before running any code, predict the factor loadings for the following hypothetical ETFs. For each, state whether you expect $\\beta_{MKT}$, $\\beta_{SMB}$, and $\\beta_{HML}$ to be positive, negative, or near zero, and briefly explain why.\n",
    "\n",
    "1. **QQQ** (Invesco Nasdaq-100 ETF) -- tracks large-cap US technology/growth stocks\n",
    "2. **IWM** (iShares Russell 2000 ETF) -- tracks US small-cap stocks (no value/growth tilt)\n",
    "3. **A leveraged 2x S&P 500 ETF**\n",
    "\n",
    "### Exercise 2: Comparing ETFs (Hands-On)\n",
    "\n",
    "Using the `fama_french_analysis()` template function defined above:\n",
    "\n",
    "1. Re-run the analysis for **IWN** (Small Cap Value) only. Is the $R^2$ higher or lower than for SPY? Why might that be?\n",
    "2. Compare the $\\beta_{SMB}$ values between SPY, IWN, and VTV. Do they match your intuition about each fund's size exposure?\n",
    "\n",
    "*Hint*: The data is already loaded in `etf_dataframes`. You can access it as `etf_dataframes['IWN']`.\n",
    "\n",
    "### Exercise 3: Sensitivity of Newey-West Standard Errors (Hands-On)\n",
    "\n",
    "The Newey-West lag length affects the standard errors. Try the following:\n",
    "\n",
    "1. Re-fit the SPY regression using `maxlags=0` (no autocorrelation correction, only heteroscedasticity). How do the standard errors compare to classic OLS?\n",
    "2. Re-fit using `maxlags=8` (more lags). How do the standard errors change compared to the default lag length?\n",
    "3. What does this tell you about the sensitivity of your conclusions to lag choice?\n",
    "\n",
    "```python\n",
    "# Starter code for Exercise 3:\n",
    "model_ex3 = sm.OLS(df['Stock_Excess_Return'].values, sm.add_constant(df[['Mkt_RF', 'SMB', 'HML']].values))\n",
    "results_ex3 = model_ex3.fit()\n",
    "\n",
    "# Try different lag lengths:\n",
    "results_lag0 = results_ex3.get_robustcov_results(cov_type='HAC', maxlags=0)\n",
    "results_lag8 = results_ex3.get_robustcov_results(cov_type='HAC', maxlags=8)\n",
    "\n",
    "print(\"Lag 0 SEs:\", results_lag0.bse)\n",
    "print(\"Lag 8 SEs:\", results_lag8.bse)\n",
    "```\n",
    "\n",
    "### Exercise 4: Economic vs. Statistical Significance (Discussion)\n",
    "\n",
    "Suppose you find that $\\hat{\\alpha} = 0.0003$ (0.03% per month) with a p-value of 0.04.\n",
    "\n",
    "1. Is this result statistically significant at the 5% level?\n",
    "2. Is it economically significant? (Hint: What does 0.03% per month translate to annually? Would an investor change their portfolio based on this?)\n",
    "3. What might happen to the p-value if you used Newey-West standard errors instead of classic OLS?\n",
    "\n",
    "### Exercise 5: Extending to Another Asset (Challenge)\n",
    "\n",
    "Download return data for a stock or ETF of your choice and run the full Fama-French 3-factor analysis:\n",
    "\n",
    "1. Download the data using `yfinance`\n",
    "2. Compute monthly excess returns\n",
    "3. Run the regression with Newey-West standard errors\n",
    "4. Interpret the coefficients -- what does the asset's factor exposure tell you about its risk characteristics?\n",
    "5. Check the diagnostic plots -- are the regression assumptions satisfied?\n",
    "\n",
    "```python\n",
    "# Starter code for Exercise 5:\n",
    "# Replace 'AAPL' with any US-listed ticker\n",
    "import yfinance as yf\n",
    "my_data = yf.download('AAPL', start='2010-01-01', end='2023-12-31', progress=False)\n",
    "# ... compute monthly returns, align with ff_factors, and run fama_french_analysis()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By working through this tutorial, you've learned:\n",
    "\n",
    "- **Statistical foundations**: How multiple linear regression works from the ground up  \n",
    "- **Mathematical derivations**: The normal equations and OLS formula\n",
    "- **Practical application**: How to fit models and make predictions in Python  \n",
    "- **Interpretation skills**: How to understand and communicate results\n",
    "- **Diagnostic expertise**: How to validate assumptions and troubleshoot problems\n",
    "- **Robust inference**: When and how to use Newey-West and other robust standard error methods\n",
    "- **Financial intuition**: How the Fama-French model illuminates investment returns\n",
    "\n",
    "### What Makes You Ready\n",
    "\n",
    "You now understand that regression is not \"black box\" machine learning -- it's a principled statistical method where:\n",
    "- **Every estimate has uncertainty** (standard errors and confidence intervals)\n",
    "- **Standard errors matter as much as coefficients** (violated assumptions can bias SEs even if coefficients are correct)\n",
    "- **Heteroscedasticity and autocorrelation are real concerns** (especially in financial time-series)\n",
    "- **Robust methods are standard practice** (not optional in professional work)\n",
    "- **Statistical significance is not the same as economic significance** (a tiny effect might be statistically significant)\n",
    "- **Assumptions matter** (violations don't invalidate everything, but they matter)\n",
    "- **Context is crucial** (data source, time period, definitions, and autocorrelation structure all matter)\n",
    "\n",
    "### Quick Reference: Robust Standard Errors\n",
    "\n",
    "| Situation | Use This | Why |\n",
    "|-----------|----------|-----|\n",
    "| Cross-sectional data (different units, one time) | HC1 or HC3 | Corrects for heteroscedasticity |\n",
    "| Time-series data (same unit over time) | Newey-West HAC | Corrects for both heteroscedasticity AND autocorrelation |\n",
    "| Financial time-series (monthly/daily returns) | **Newey-West** | Standard approach in finance research |\n",
    "| Clustered data (e.g., firms in sectors) | Clustered SE | Accounts for within-cluster correlation |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try it yourself**: Analyze different stocks or ETFs using the template function above with **Newey-West standard errors**\n",
    "2. **Understand the lag selection**: Be able to explain why you chose your lag length\n",
    "3. **Compare robustness**: Run HC1 and Newey-West; if they agree, great; if they differ, investigate\n",
    "4. **Explore extensions**: \n",
    "   - Try the 5-factor model \n",
    "   - Use rolling windows to see time-variation\n",
    "   - Compare results across different lag lengths\n",
    "5. **Deepen your understanding**: \n",
    "   - Study the original Fama & French papers\n",
    "   - Read Newey & West (1987) on HAC standard errors\n",
    "   - Explore time-series dynamics and GARCH models\n",
    "6. **Apply to other domains**: These same techniques apply to economics, medicine, social science, and beyond\n",
    "\n",
    "### Final Words\n",
    "\n",
    "Statistics and data analysis are powerful tools, but they come with responsibility. Always:\n",
    "- Check your work\n",
    "- Report the standard error method you used\n",
    "- Consider alternative explanations\n",
    "- Be honest about limitations\n",
    "- Report uncertainty properly\n",
    "- Think about what's economically meaningful, not just statistically significant\n",
    "- Use robust methods as a matter of course in empirical work\n",
    "\n",
    "**In finance research especially**: Always use robust standard errors for time-series regressions like Fama-French. It's not just good practice -- it's expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
