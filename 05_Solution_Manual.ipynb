{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09da3ef8",
   "metadata": {},
   "source": [
    "# Solution Manual — Fama-French Tutorial Series\n",
    "\n",
    "## Complete Worked Solutions to All Exercises\n",
    "\n",
    "This notebook provides **detailed solutions** to every exercise from the four tutorial notebooks:\n",
    "\n",
    "1. [Statistical Foundations](01_Statistical_Foundations.ipynb) — 5 exercises\n",
    "2. [Time-Series Foundations](02_Time_Series_Foundations.ipynb) — 4 exercises\n",
    "3. [Fama-French 3-Factor Model](03_Fama_French_3Factor.ipynb) — 5 exercises\n",
    "4. [Advanced Factor Models](04_Advanced_Factor_Models.ipynb) — 3 exercises\n",
    "\n",
    "**How to use this notebook:**\n",
    "- Try each exercise yourself first before looking at the solution\n",
    "- Each solution includes the code, the output interpretation, and an explanation of **why** the answer is what it is\n",
    "- Discussion-type exercises include model answers that you can compare against your own reasoning\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** Run this notebook **after** working through notebooks 01–04. The data downloads and library imports below mirror those tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9614434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Setup: Import all libraries needed across all solutions\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n",
    "import yfinance as yf\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def get_close(raw, ticker=None):\n",
    "    \"\"\"Extract Close price from yf.download result, handling MultiIndex columns.\"\"\"\n",
    "    if isinstance(raw.columns, pd.MultiIndex):\n",
    "        try:\n",
    "            return raw[('Close', ticker)]\n",
    "        except KeyError:\n",
    "            return raw.xs('Close', axis=1, level=0).iloc[:, 0]\n",
    "    elif 'Close' in raw.columns:\n",
    "        return raw['Close']\n",
    "    else:\n",
    "        return raw.iloc[:, 0]\n",
    "\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f72681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Helper: Download and parse a CSV from Kenneth French's data library\n",
    "# (reused from Notebook 04)\n",
    "# ============================================================================\n",
    "\n",
    "def download_ff_csv(zip_filename):\n",
    "    \"\"\"Download and parse a CSV from Kenneth French's data library.\"\"\"\n",
    "    url = f\"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/{zip_filename}\"\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        zip_path = f\"{tmpdir}/data.zip\"\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(tmpdir)\n",
    "        files = [f for f in os.listdir(tmpdir) if f.lower().endswith('.csv')]\n",
    "        if not files:\n",
    "            raise ValueError(f\"No CSV found in {zip_filename}\")\n",
    "        csv_path = f\"{tmpdir}/{files[0]}\"\n",
    "        \n",
    "        with open(csv_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        skip = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            stripped = line.strip()\n",
    "            if re.match(r'^\\d{6}', stripped):\n",
    "                skip = i\n",
    "                break\n",
    "        \n",
    "        header_line = skip - 1 if skip > 0 else 0\n",
    "        df = pd.read_csv(csv_path, skiprows=header_line, index_col=0)\n",
    "    \n",
    "    df.index = pd.Index(df.index.astype(str).str.strip())\n",
    "    df = df[df.index.str.match(r'^\\d{6}$')]\n",
    "    df.index = pd.to_datetime(df.index, format='%Y%m')\n",
    "    df = df.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "    df = df / 100  # Convert percentages to decimals\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_ff_regression(y, factor_names, data, name=\"Asset\"):\n",
    "    \"\"\"Run a Fama-French regression with Newey-West SEs and print results.\"\"\"\n",
    "    X = sm.add_constant(data[factor_names].values)\n",
    "    model = sm.OLS(y if y is not None else data.iloc[:, 0].values, X).fit()\n",
    "    n = len(data)\n",
    "    lag = int(np.floor(4 * (n / 100) ** (2/9)))\n",
    "    robust = model.get_robustcov_results(cov_type='HAC', maxlags=lag)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {name}  (Newey-West, lag={lag})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(robust.summary().tables[1])\n",
    "    print(f\"  R² = {robust.rsquared:.4f}   Adj-R² = {robust.rsquared_adj:.4f}\")\n",
    "    return robust\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Download all data needed for solutions\n",
    "# ============================================================================\n",
    "\n",
    "# --- Fama-French 3-Factor (2000-2023, for NB02 exercises) ---\n",
    "print(\"Downloading Fama-French 3-Factor data...\")\n",
    "ff3 = download_ff_csv(\"F-F_Research_Data_Factors_CSV.zip\")\n",
    "ff3.columns = ['Mkt_RF', 'SMB', 'HML', 'RF']\n",
    "ff3_full = ff3.loc['2000-01-01':'2023-12-31'].copy()\n",
    "# Also keep a 2010-2023 slice for NB03 exercises\n",
    "ff3_short = ff3.loc['2010-01-01':'2023-12-31'].copy()\n",
    "\n",
    "# --- Fama-French 5-Factor ---\n",
    "print(\"Downloading Fama-French 5-Factor data...\")\n",
    "ff5 = download_ff_csv(\"F-F_Research_Data_5_Factors_2x3_CSV.zip\")\n",
    "ff5.columns = ['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "ff5 = ff5.loc['2000-01-01':'2023-12-31']\n",
    "\n",
    "# --- Momentum ---\n",
    "print(\"Downloading Momentum factor...\")\n",
    "mom = download_ff_csv(\"F-F_Momentum_Factor_CSV.zip\")\n",
    "mom.columns = ['UMD']\n",
    "mom = mom.loc['2000-01-01':'2023-12-31']\n",
    "\n",
    "# --- Combine all factors ---\n",
    "factors = ff5.drop(columns=['RF']).join(mom, how='inner')\n",
    "\n",
    "print(f\"\\nAll data loaded.\")\n",
    "print(f\"  3-Factor (full):  {len(ff3_full)} months  ({ff3_full.index[0].date()} – {ff3_full.index[-1].date()})\")\n",
    "print(f\"  3-Factor (short): {len(ff3_short)} months ({ff3_short.index[0].date()} – {ff3_short.index[-1].date()})\")\n",
    "print(f\"  5-Factor + Mom:   {len(factors)} months\")\n",
    "print(f\"  Factor columns:   {list(factors.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d235176",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Statistical Foundations (Notebook 01) — Solutions\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Hand Computation\n",
    "\n",
    "**Problem:** Given $x = [2, 5, 3, 8, 7]$ and $y = [4, 7, 5, 10, 9]$:\n",
    "1. Compute mean, variance, std dev of $x$ and $y$\n",
    "2. Compute $\\text{Cov}(x,y)$ and $\\text{Corr}(x,y)$\n",
    "3. Set up the design matrix and compute $\\hat\\beta$ via the normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32155ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB01 Exercise 1 — Hand Computation (verified with NumPy)\n",
    "# ============================================================================\n",
    "\n",
    "x = np.array([2, 5, 3, 8, 7], dtype=float)\n",
    "y = np.array([4, 7, 5, 10, 9], dtype=float)\n",
    "n = len(x)\n",
    "\n",
    "# ── Part 1: Mean, Variance, Standard Deviation ──\n",
    "mean_x = x.sum() / n\n",
    "mean_y = y.sum() / n\n",
    "\n",
    "var_x = ((x - mean_x)**2).sum() / (n - 1)   # sample variance (Bessel's correction)\n",
    "var_y = ((y - mean_y)**2).sum() / (n - 1)\n",
    "\n",
    "std_x = np.sqrt(var_x)\n",
    "std_y = np.sqrt(var_y)\n",
    "\n",
    "print(\"PART 1: Descriptive Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  x = {x}\")\n",
    "print(f\"  y = {y}\")\n",
    "print(f\"\\n  mean(x) = {x.sum()}/{n} = {mean_x:.2f}\")\n",
    "print(f\"  mean(y) = {y.sum()}/{n} = {mean_y:.2f}\")\n",
    "print(f\"\\n  var(x)  = Σ(xᵢ − x̄)² / (n−1) = {((x - mean_x)**2).sum():.2f} / {n-1} = {var_x:.4f}\")\n",
    "print(f\"  var(y)  = {((y - mean_y)**2).sum():.2f} / {n-1} = {var_y:.4f}\")\n",
    "print(f\"\\n  std(x)  = √{var_x:.4f} = {std_x:.4f}\")\n",
    "print(f\"  std(y)  = √{var_y:.4f} = {std_y:.4f}\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\n  Verify with NumPy:  std(x, ddof=1) = {x.std(ddof=1):.4f}  ✓\")\n",
    "\n",
    "# ── Part 2: Covariance and Correlation ──\n",
    "cov_xy = ((x - mean_x) * (y - mean_y)).sum() / (n - 1)\n",
    "corr_xy = cov_xy / (std_x * std_y)\n",
    "\n",
    "print(f\"\\nPART 2: Covariance and Correlation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Cov(x,y) = Σ(xᵢ-x̄)(yᵢ-ȳ)/(n-1) = {((x-mean_x)*(y-mean_y)).sum():.2f}/{n-1} = {cov_xy:.4f}\")\n",
    "print(f\"  Corr(x,y) = Cov(x,y)/(s_x · s_y) = {cov_xy:.4f}/({std_x:.4f}·{std_y:.4f}) = {corr_xy:.4f}\")\n",
    "print(f\"\\n  Verify: np.corrcoef = {np.corrcoef(x, y)[0, 1]:.4f}  ✓\")\n",
    "\n",
    "# ── Part 3: Normal Equations ──\n",
    "X = np.column_stack([np.ones(n), x])   # design matrix with intercept\n",
    "XtX     = X.T @ X\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "Xty     = X.T @ y\n",
    "beta_hat = XtX_inv @ Xty\n",
    "\n",
    "print(f\"\\nPART 3: OLS via Normal Equations\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Design matrix X (n×2):\\n{X}\")\n",
    "print(f\"\\n  X'X =\\n{XtX}\")\n",
    "print(f\"\\n  (X'X)⁻¹ =\\n{XtX_inv}\")\n",
    "print(f\"\\n  X'y = {Xty}\")\n",
    "print(f\"\\n  β̂ = (X'X)⁻¹ X'y = [{beta_hat[0]:.6f}, {beta_hat[1]:.6f}]\")\n",
    "print(f\"\\n  Interpretation:  ŷ = {beta_hat[0]:.4f} + {beta_hat[1]:.4f} · x\")\n",
    "print(f\"    → intercept ≈ {beta_hat[0]:.2f}, slope ≈ {beta_hat[1]:.2f}\")\n",
    "\n",
    "# Quick sanity check: slope should be close to Cov(x,y)/Var(x)\n",
    "print(f\"\\n  Sanity: Cov(x,y)/Var(x) = {cov_xy/var_x:.6f}  (should match slope)  ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fbb394",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: CLT in Action — Beta(0.5, 0.5) Distribution\n",
    "\n",
    "**Problem:** Repeat the CLT experiment with $\\text{Beta}(0.5, 0.5)$ — a **bimodal** distribution. Show that sample means become normal as $n$ grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB01 Exercise 2 — CLT with Beta(0.5, 0.5)\n",
    "# ============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "n_sims = 5000\n",
    "a, b = 0.5, 0.5\n",
    "mu_beta  = a / (a + b)                          # = 0.5\n",
    "var_beta = (a * b) / ((a + b)**2 * (a + b + 1)) # = 0.125\n",
    "\n",
    "sample_sizes = [5, 30, 100]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(sample_sizes) + 1, figsize=(18, 4))\n",
    "\n",
    "# Panel 0: Show the raw Beta(0.5, 0.5) density (bimodal)\n",
    "x_grid = np.linspace(0.001, 0.999, 500)\n",
    "axes[0].plot(x_grid, stats.beta.pdf(x_grid, a, b), 'k-', linewidth=2)\n",
    "axes[0].set_title('Beta(0.5, 0.5) density\\n(bimodal!)', fontweight='bold')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panels 1–3: Histograms of standardised sample means\n",
    "for i, n in enumerate(sample_sizes):\n",
    "    means = np.array([np.random.beta(a, b, size=n).mean() for _ in range(n_sims)])\n",
    "    z = (means - mu_beta) / np.sqrt(var_beta / n)   # standardise to N(0,1) if CLT holds\n",
    "    \n",
    "    ax = axes[i + 1]\n",
    "    ax.hist(z, bins=50, density=True, alpha=0.6, color='steelblue', edgecolor='white')\n",
    "    \n",
    "    # Overlay N(0,1) density\n",
    "    t_grid = np.linspace(-4, 4, 200)\n",
    "    ax.plot(t_grid, stats.norm.pdf(t_grid), 'r-', linewidth=2, label='N(0,1)')\n",
    "    \n",
    "    ax.set_title(f'n = {n}', fontweight='bold', fontsize=13)\n",
    "    ax.set_xlabel('Standardised z')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('CLT Demonstration: Beta(0.5, 0.5) — Sample Means Become Normal',\n",
    "             fontweight='bold', fontsize=13, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── Discussion answers ──\n",
    "print(\"ANSWERS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Q3: At what n does the distribution look approximately normal?\n",
    "   Even at n = 5 the histogram is already quite close to the\n",
    "   N(0,1) bell curve. By n = 30 it is virtually indistinguishable.\n",
    "   This is FASTER than for Exponential(1).\n",
    "\n",
    "Q4: Why does a bimodal distribution converge faster?\n",
    "   The Beta(0.5, 0.5) distribution is SYMMETRIC around its mean\n",
    "   (μ = 0.5). Its third central moment (skewness) is exactly zero.\n",
    "   The CLT's convergence rate depends heavily on skewness: the\n",
    "   Berry-Esseen bound is proportional to E[|X − μ|³] / σ³.\n",
    "   Because the Beta(0.5,0.5) has zero skewness, the leading error\n",
    "   term in the normal approximation vanishes, giving much faster\n",
    "   convergence than the heavily right-skewed Exponential(1).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c271555",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Sandwich Estimator Comparison\n",
    "\n",
    "**Problem:** Take the 5-observation dataset from NB01 Section 5, inflate $y$ for the last two observations by a factor of 3 to create heteroscedasticity, and compare Classic, HC0, and Newey-West standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB01 Exercise 3 — Sandwich Estimator with Artificial Heteroscedasticity\n",
    "# ============================================================================\n",
    "\n",
    "# Original 5-observation dataset from NB01 Section 5\n",
    "x_orig = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_orig = np.array([2.1, 3.9, 6.2, 7.8, 10.1])\n",
    "\n",
    "# Step 1: Inflate last two y-values by factor 3\n",
    "multiplier = np.array([1, 1, 1, 3, 3], dtype=float)\n",
    "y_het = y_orig * multiplier\n",
    "\n",
    "print(\"STEP 1: Create heteroscedastic data\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  y_original  = {y_orig}\")\n",
    "print(f\"  multiplier  = {multiplier}\")\n",
    "print(f\"  y_inflated  = {y_het}\")\n",
    "\n",
    "# Step 2: Fit OLS on the inflated data\n",
    "X_ex3 = sm.add_constant(x_orig)\n",
    "model_ex3 = sm.OLS(y_het, X_ex3).fit()\n",
    "\n",
    "# Classic SE\n",
    "se_classic = model_ex3.bse\n",
    "\n",
    "# HC0\n",
    "model_hc0 = model_ex3.get_robustcov_results(cov_type='HC0')\n",
    "se_hc0 = model_hc0.bse\n",
    "\n",
    "# Newey-West (L=1 for n=5)\n",
    "model_nw = model_ex3.get_robustcov_results(cov_type='HAC', maxlags=1)\n",
    "se_nw = model_nw.bse\n",
    "\n",
    "print(f\"\\nSTEP 2: OLS on inflated data\")\n",
    "print(f\"  β̂₀ (intercept) = {model_ex3.params[0]:.4f}\")\n",
    "print(f\"  β̂₁ (slope)     = {model_ex3.params[1]:.4f}\")\n",
    "\n",
    "print(f\"\\nSTEP 3: Compare Standard Errors for β̂₁ (slope)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  {'Method':<20s}  {'SE(β̂₁)':>10s}  {'% vs Classic':>14s}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "print(f\"  {'Classic OLS':<20s}  {se_classic[1]:10.4f}  {'(baseline)':>14s}\")\n",
    "print(f\"  {'HC0':<20s}  {se_hc0[1]:10.4f}  {(se_hc0[1]/se_classic[1]-1)*100:+13.1f}%\")\n",
    "print(f\"  {'Newey-West (L=1)':<20s}  {se_nw[1]:10.4f}  {(se_nw[1]/se_classic[1]-1)*100:+13.1f}%\")\n",
    "\n",
    "print(f\"\"\"\n",
    "INTERPRETATION\n",
    "{'='*60}\n",
    "The HC0 standard error changes the most relative to classic OLS.\n",
    "\n",
    "Why? Classic OLS assumes Var(εᵢ) = σ² for all i — a single \n",
    "variance for every observation. But by inflating the last two \n",
    "y-values, we made the residuals for observations 4 and 5 much \n",
    "larger, violating homoscedasticity.\n",
    "\n",
    "HC0 replaces the single σ² with observation-specific ε̂ᵢ², so \n",
    "it correctly accounts for the larger residuals at the end. The \n",
    "Newey-West estimator also picks this up (it nests HC0 at lag 0) \n",
    "but with only 5 observations the additional autocorrelation \n",
    "correction is negligible.\n",
    "\n",
    "With n = 5, ALL estimators are unreliable — this is just to \n",
    "show the mechanics. With realistic sample sizes (n ≥ 100), the \n",
    "differences become both larger and more trustworthy.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743026f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Monte Carlo — Varying the Autocorrelation (Challenge)\n",
    "\n",
    "**Problem:** Run Monte Carlo simulations with $\\rho \\in \\{0, 0.3, 0.6, 0.9\\}$ and plot rejection rates for Classic OLS vs Newey-West."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB01 Exercise 4 — Monte Carlo: Rejection Rate vs Autocorrelation\n",
    "# ============================================================================\n",
    "\n",
    "np.random.seed(0)\n",
    "num_mc = 2000\n",
    "n_mc = 150\n",
    "rho_values = [0.0, 0.3, 0.6, 0.9]\n",
    "\n",
    "results_mc = []  # list of (rho, classic_rate, nw_rate)\n",
    "\n",
    "for rho in rho_values:\n",
    "    reject_classic = 0\n",
    "    reject_nw = 0\n",
    "    \n",
    "    for sim in range(num_mc):\n",
    "        x_mc = np.random.randn(n_mc) * 0.04\n",
    "        \n",
    "        # AR(1) errors with heteroscedasticity and fat tails\n",
    "        innov = stats.t.rvs(df=5, size=n_mc)\n",
    "        eps = np.zeros(n_mc)\n",
    "        for t in range(n_mc):\n",
    "            eps[t] = rho * eps[t-1] + innov[t] if t > 0 else innov[t]\n",
    "        eps = eps * (0.01 + 0.4 * np.abs(x_mc))\n",
    "        \n",
    "        y_mc = 0.0 + 0.0 * x_mc + eps   # true β = 0\n",
    "        \n",
    "        X_mc = sm.add_constant(x_mc)\n",
    "        res_mc = sm.OLS(y_mc, X_mc).fit()\n",
    "        lag_mc = int(np.floor(4 * (n_mc / 100) ** (2/9)))\n",
    "        res_nw_mc = res_mc.get_robustcov_results(cov_type='HAC', maxlags=lag_mc)\n",
    "        \n",
    "        if res_mc.pvalues[1] < 0.05:\n",
    "            reject_classic += 1\n",
    "        if res_nw_mc.pvalues[1] < 0.05:\n",
    "            reject_nw += 1\n",
    "    \n",
    "    rate_c = reject_classic / num_mc * 100\n",
    "    rate_nw = reject_nw / num_mc * 100\n",
    "    results_mc.append((rho, rate_c, rate_nw))\n",
    "    print(f\"  ρ = {rho:.1f}:  Classic = {rate_c:.1f}%,  Newey-West = {rate_nw:.1f}%\")\n",
    "\n",
    "# ── Plot ──\n",
    "rhos    = [r[0] for r in results_mc]\n",
    "classic = [r[1] for r in results_mc]\n",
    "nw      = [r[2] for r in results_mc]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(rhos, classic, 'o-', color='#d9534f', linewidth=2, markersize=8, label='Classic OLS')\n",
    "ax.plot(rhos, nw,      's-', color='#5cb85c', linewidth=2, markersize=8, label='Newey-West HAC')\n",
    "ax.axhline(5, color='black', linestyle='--', linewidth=1.5, label='Nominal 5% level')\n",
    "ax.set_xlabel('Autocorrelation coefficient ρ', fontsize=12)\n",
    "ax.set_ylabel('Rejection rate (%)', fontsize=12)\n",
    "ax.set_title('False Rejection Rate Under H₀ vs Autocorrelation Strength\\n'\n",
    "             f'({num_mc} simulations, n={n_mc}, heteroscedastic t(5) errors)',\n",
    "             fontweight='bold', fontsize=12)\n",
    "ax.set_xticks(rho_values)\n",
    "ax.set_ylim(0, max(classic) * 1.25)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "INTERPRETATION\n",
    "{'='*60}\n",
    "• At ρ = 0 (no autocorrelation), classic OLS is already slightly\n",
    "  above 5% because of heteroscedasticity alone.\n",
    "\n",
    "• By ρ = 0.3, classic OLS rejects roughly {classic[1]:.0f}% of the time\n",
    "  — already a meaningful over-rejection.\n",
    "\n",
    "• At ρ = 0.6, classic OLS rejects at ~{classic[2]:.0f}% — you'd be \"finding\"\n",
    "  significant results roughly {classic[2]/5:.0f}× more often than you should.\n",
    "\n",
    "• At ρ = 0.9, classic OLS is wildly unreliable ({classic[3]:.0f}% rejection).\n",
    "\n",
    "• Newey-West stays close to 5% across all ρ values, confirming\n",
    "  that the HAC correction works as advertised.\n",
    "\n",
    "CONCLUSION: Even moderate autocorrelation (ρ ≈ 0.3) is enough\n",
    "to make classic OLS seriously misleading. Always use Newey-West\n",
    "for time-series regressions.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb5305",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Why Does the Mixing CLT Matter? (Discussion)\n",
    "\n",
    "### Model Answer\n",
    "\n",
    "**Q1. Why can't the standard CLT (Lindeberg-Lévy) be applied directly to time-series data?**\n",
    "\n",
    "The Lindeberg-Lévy CLT requires the observations $X_1, X_2, \\ldots, X_n$ to be **independent and identically distributed (i.i.d.)**. In time-series data, consecutive observations are typically correlated — today's stock return is not independent of yesterday's. This dependence violates the independence assumption. The standard CLT doesn't apply because its proof relies on independence to decompose the variance of the sum into a sum of variances: $\\text{Var}(\\sum X_i) = \\sum \\text{Var}(X_i)$. With dependence, cross-covariance terms $\\text{Cov}(X_i, X_j)$ appear and the decomposition breaks.\n",
    "\n",
    "**Q2. What additional condition does the Mixing CLT require, and why is it satisfied for financial returns?**\n",
    "\n",
    "The Mixing CLT requires that the **dependence between observations decays sufficiently fast** as they get further apart in time. Formally, the $\\alpha$-mixing coefficients (which measure the maximum dependence between the \"past\" and the \"future\" separated by a gap of $k$ periods) must converge to zero at a fast enough polynomial rate as $k \\to \\infty$.\n",
    "\n",
    "This is satisfied for financial returns because:\n",
    "- Return autocorrelations are typically very small (close to zero at lag 1, essentially zero at lag 2+), as shown empirically in Notebook 02 Section 3.\n",
    "- Volatility clustering creates dependence in **squared** returns, but the autocorrelation of squared returns also decays exponentially (as demonstrated with GARCH models).\n",
    "- The mixing condition requires only that dependence *decays* — it doesn't require zero dependence. Financial returns easily clear this bar.\n",
    "\n",
    "**Q3. If the mixing condition were NOT satisfied (e.g., a random walk), what would go wrong with Newey-West?**\n",
    "\n",
    "A random walk $P_t = P_{t-1} + \\epsilon_t$ is non-stationary: its variance grows linearly with $t$, so $\\text{Var}(P_t) = t \\sigma^2_\\epsilon$. The mixing condition fails because the process never \"forgets\" past shocks — $P_t$ depends on *all* past innovations equally.\n",
    "\n",
    "Consequences for Newey-West:\n",
    "- The **long-run variance** $\\sigma^2_{LR} = \\gamma_0 + 2 \\sum_{k=1}^{\\infty} \\gamma_k$ **diverges** (does not converge to a finite limit), because the cumulative autocovariance grows without bound.\n",
    "- The Newey-West estimator tries to estimate this (finite) long-run variance by summing Bartlett-weighted sample autocovariances up to lag $L$. But as $L$ grows with $n$, the partial sums keep growing rather than stabilising.\n",
    "- The resulting \"standard errors\" are meaningless — they don't converge to anything useful.\n",
    "- **Takeaway:** Newey-West requires stationarity (or at least a finite long-run variance). This is why we always test for stationarity (ADF test) before running regressions, and why we use **returns** (stationary) rather than **prices** (non-stationary) in financial econometrics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a66b4b",
   "metadata": {},
   "source": [
    "# Part 2: Time-Series Foundations (Notebook 02) — Solutions\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Stationarity on a New Series (AAPL)\n",
    "\n",
    "**Problem:** Download daily AAPL prices, compute log returns, run ADF tests on prices and returns, and plot rolling moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB02 Exercise 1 — Stationarity: AAPL Prices vs Log Returns\n",
    "# ============================================================================\n",
    "\n",
    "# Download AAPL data\n",
    "raw_aapl = yf.download('AAPL', start='2015-01-01', end='2023-12-31', progress=False)\n",
    "if isinstance(raw_aapl.columns, pd.MultiIndex):\n",
    "    prices = raw_aapl[('Close', 'AAPL')]\n",
    "else:\n",
    "    prices = raw_aapl['Close']\n",
    "prices = prices.dropna()\n",
    "\n",
    "log_returns = np.log(prices / prices.shift(1)).dropna()\n",
    "\n",
    "# ── Part 1: ADF tests ──\n",
    "print(\"ADF TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "adf_price = adfuller(prices.values, maxlag=20, autolag='AIC')\n",
    "print(f\"\\nAAPL PRICES:\")\n",
    "print(f\"  ADF statistic:  {adf_price[0]:.4f}\")\n",
    "print(f\"  p-value:        {adf_price[1]:.4f}\")\n",
    "print(f\"  Conclusion:     {'STATIONARY' if adf_price[1] < 0.05 else 'NON-STATIONARY'}\")\n",
    "\n",
    "adf_ret = adfuller(log_returns.values, maxlag=20, autolag='AIC')\n",
    "print(f\"\\nAAPL LOG RETURNS:\")\n",
    "print(f\"  ADF statistic:  {adf_ret[0]:.4f}\")\n",
    "print(f\"  p-value:        {adf_ret[1]:.6f}\")\n",
    "print(f\"  Conclusion:     {'STATIONARY' if adf_ret[1] < 0.05 else 'NON-STATIONARY'}\")\n",
    "\n",
    "print(f\"\\nDo results match Fama-French findings?\")\n",
    "print(f\"  YES — prices are non-stationary (unit root), returns are stationary.\")\n",
    "print(f\"  This is universal for financial assets, not specific to any one stock.\")\n",
    "\n",
    "# ── Part 2: Rolling 60-day mean and std dev ──\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "axes[0, 0].plot(prices.index, prices, color='darkblue', linewidth=0.8)\n",
    "axes[0, 0].set_title('AAPL Daily Close Price', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Price ($)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(log_returns.index, log_returns * 100, color='steelblue', linewidth=0.5, alpha=0.7)\n",
    "axes[0, 1].set_title('AAPL Daily Log Returns', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Return (%)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "rolling_mean = log_returns.rolling(60).mean() * 100\n",
    "axes[1, 0].plot(rolling_mean.index, rolling_mean, color='darkred', linewidth=1)\n",
    "axes[1, 0].axhline(log_returns.mean() * 100, color='black', linestyle='--', alpha=0.5,\n",
    "                     label=f'Full-sample mean = {log_returns.mean()*100:.3f}%')\n",
    "axes[1, 0].set_title('Rolling 60-Day Mean of Log Returns', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Mean return (%)')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "rolling_std = log_returns.rolling(60).std() * 100\n",
    "axes[1, 1].plot(rolling_std.index, rolling_std, color='darkorange', linewidth=1)\n",
    "axes[1, 1].axhline(log_returns.std() * 100, color='black', linestyle='--', alpha=0.5,\n",
    "                     label=f'Full-sample std = {log_returns.std()*100:.3f}%')\n",
    "axes[1, 1].set_title('Rolling 60-Day Std Dev (Volatility)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Std dev (%)')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('AAPL: Stationarity Analysis', fontweight='bold', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION\n",
    "  The rolling mean fluctuates around the full-sample mean and doesn't\n",
    "  show a systematic trend — consistent with stationarity in the mean.\n",
    "  \n",
    "  The rolling std dev varies substantially (volatility clustering), but\n",
    "  this is TIME-VARYING VOLATILITY, not non-stationarity in the traditional\n",
    "  sense. The series is still covariance-stationary to a reasonable\n",
    "  approximation, as confirmed by the ADF test.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370d531",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Interpreting ACF Plots (Conceptual)\n",
    "\n",
    "**Problem:** For (A) white noise, (B) AR(1) with φ=0.8, and (C) financial returns with GARCH volatility, describe the ACF of $X_t$ and $X_t^2$, and explain why (C) makes Newey-West necessary.\n",
    "\n",
    "### Model Answer\n",
    "\n",
    "| Process | ACF of $X_t$ | ACF of $X_t^2$ |\n",
    "|---------|---------------|-----------------|\n",
    "| **(A) White noise** | All zero beyond lag 0. The ACF plot shows no bars exceeding the 95% confidence band. | Also all zero — $X_t^2$ is i.i.d. $\\chi^2(1)$ (rescaled), so its ACF is also flat. |\n",
    "| **(B) AR(1), φ = 0.8** | Exponential decay: $\\rho_k = 0.8^k$. Significant at many lags, slowly dying out. | Also shows significant autocorrelation (because $X_t$ is autocorrelated, so is $X_t^2$), decaying roughly as $\\rho_k^2 \\approx 0.64^k$. |\n",
    "| **(C) Financial returns** | Nearly zero at all lags — looks almost identical to (A). Perhaps a tiny significant value at lag 1, but mostly within the bands. | **Highly significant at many lags** — strong, slowly decaying autocorrelation. This is the hallmark of **volatility clustering**: large returns (of either sign) tend to follow large returns. |\n",
    "\n",
    "**Why does (C) make Newey-West necessary?**\n",
    "\n",
    "Process (C) is the tricky one. Looking at the ACF of *returns*, you'd conclude \"basically white noise, no correction needed.\" But this is misleading. The returns are **uncorrelated but not independent** — their conditional variance is serially dependent (GARCH structure). This dependence affects the variance of OLS estimates in a subtle way:\n",
    "\n",
    "- The classic OLS standard error formula assumes both homoscedasticity AND no autocorrelation.\n",
    "- Process (C) violates homoscedasticity (time-varying variance = heteroscedasticity), even though it satisfies zero autocorrelation in levels.\n",
    "- **Newey-West handles both problems simultaneously**: it makes no assumption about the variance structure. Even if autocorrelation is zero, the HAC estimator correctly computes the sandwich formula using the actual squared residuals (which *are* autocorrelated).\n",
    "- In practice, financial return residuals from a factor regression often show mild autocorrelation (even if raw returns don't), because the factor model may not perfectly capture all time-series dynamics. Newey-West is insurance against this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafb00c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Effect of Sample Period on ARCH Tests\n",
    "\n",
    "**Problem:** Split Mkt-RF into 2000–2011 and 2012–2023. Run ARCH(6) tests on each and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB02 Exercise 3 — ARCH Tests on Sub-Periods\n",
    "# ============================================================================\n",
    "\n",
    "mkt_early = ff3_full['Mkt_RF'].loc[:'2011-12']\n",
    "mkt_late  = ff3_full['Mkt_RF'].loc['2012-01':]\n",
    "\n",
    "print(\"ARCH(6) TEST ON MKT-RF SUB-PERIODS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label, series in [(\"2000-2011 (includes GFC)\", mkt_early),\n",
    "                       (\"2012-2023 (post-GFC)\",     mkt_late)]:\n",
    "    lm_stat, lm_pval, fstat, f_pval = het_arch(series.values, nlags=6)\n",
    "    print(f\"\\n  {label}\")\n",
    "    print(f\"  {'─'*50}\")\n",
    "    print(f\"  Number of months:  {len(series)}\")\n",
    "    print(f\"  Monthly std dev:   {series.std()*100:.2f}%\")\n",
    "    print(f\"  LM statistic:      {lm_stat:.4f}\")\n",
    "    print(f\"  LM p-value:        {lm_pval:.4f}\")\n",
    "    sig = \"SIGNIFICANT\" if lm_pval < 0.05 else \"NOT significant\"\n",
    "    print(f\"  Conclusion:        {sig} volatility clustering\")\n",
    "\n",
    "# Visualise\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4.5))\n",
    "\n",
    "for ax, (label, series) in zip(axes, [(\"2000–2011 (GFC era)\", mkt_early),\n",
    "                                        (\"2012–2023 (post-GFC)\", mkt_late)]):\n",
    "    ax.plot(series.index, series * 100, linewidth=0.8, color='darkblue')\n",
    "    ax.fill_between(series.index, series * 100, 0, alpha=0.3, color='steelblue')\n",
    "    ax.set_title(f'Mkt-RF: {label}', fontweight='bold')\n",
    "    ax.set_ylabel('Return (%)')\n",
    "    ax.set_ylim(-25, 20)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION\n",
    "  The 2000–2011 period includes the dot-com bust (2000–2002) and the\n",
    "  Global Financial Crisis (2008–2009) — two episodes of extreme\n",
    "  volatility clustering. The ARCH test statistic is likely much\n",
    "  larger (stronger evidence of volatility clustering).\n",
    "\n",
    "  The 2012–2023 period is calmer overall, though it includes the\n",
    "  COVID crash (March 2020). Volatility clustering is typically\n",
    "  still present but weaker.\n",
    "\n",
    "  KEY INSIGHT: Volatility clustering is ubiquitous in financial\n",
    "  data, but its STRENGTH varies with the sample period. This is\n",
    "  why Newey-West standard errors are always recommended regardless\n",
    "  of what any single-period test shows.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42c477",
   "metadata": {},
   "source": [
    "### Exercise 4 — When Does the Mixing Condition Fail in Practice?\n",
    "\n",
    "**Problem:** Name two realistic scenarios where financial return data might violate the mixing (weak dependence) condition required for the CLT and Newey-West consistency.\n",
    "\n",
    "---\n",
    "\n",
    "**Model Answer:**\n",
    "\n",
    "1. **Structural Breaks / Regime Changes**\n",
    "   When the data-generating process itself changes — e.g., a central bank switches from inflation targeting to yield-curve control, or a market transitions from floor trading to electronic trading — the return series is no longer a single stationary, ergodic process. Early observations carry no information about the new regime, so the \"forgetting\" property that mixing requires is violated. In finite samples the Newey-West estimator uses autocovariances from the old regime to estimate uncertainty in the new one, which can be wildly wrong.\n",
    "\n",
    "2. **Unit-Root or Near-Unit-Root Behaviour**\n",
    "   Asset *prices* follow a near-random walk (the ratio $\\operatorname{Var}[\\bar{X}_n] / (\\sigma^2/n) \\to \\infty$). If a researcher accidentally regresses *price levels* instead of *returns*, the mixing condition fails because shocks never decay. Even for returns, very persistent predictors such as log dividend-price ratios ($\\rho \\approx 0.99$ monthly) can produce near-unit-root regressors whose mixing rate is too slow for the bandwidth available in a typical sample.\n",
    "\n",
    "3. **Long Memory / Fractional Integration (Bonus)**\n",
    "   Realised-volatility measures often exhibit long memory ($d \\approx 0.4$). Autocovariances decay hyperbolically rather than exponentially, so the strong-mixing coefficients $\\alpha(k)$ decay too slowly for the standard Newey-West theory. Special long-run variance estimators (e.g., VARHAC or fixed-$b$ asymptotics) are needed in this case.\n",
    "\n",
    "**Practical take-away:** Always plot the data, run rolling-window checks, and test for structural breaks (Chow, CUSUM) before trusting any single asymptotic standard-error estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097a688",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — Fama-French 3-Factor Model (Notebook 03)\n",
    "\n",
    "### Exercise 1 — Predicting Factor Loadings (Conceptual)\n",
    "\n",
    "**Problem:** Before running any code, predict the factor loadings ($\\beta_{\\text{MKT}}$, $\\beta_{\\text{SMB}}$, $\\beta_{\\text{HML}}$) for QQQ, IWM, and a 2× leveraged S&P 500 ETF.\n",
    "\n",
    "---\n",
    "\n",
    "**Model Answer:**\n",
    "\n",
    "| ETF | $\\beta_{\\text{MKT}}$ | $\\beta_{\\text{SMB}}$ | $\\beta_{\\text{HML}}$ | Reasoning |\n",
    "|---|---|---|---|---|\n",
    "| **QQQ** | ≈ 1.0–1.1 | Slightly **negative** | **Negative** (strong) | Nasdaq-100 is large-cap (negative SMB) and heavily tilted toward growth stocks (negative HML, since HML is *value minus growth*). |\n",
    "| **IWM** | ≈ 1.0–1.1 | **Positive** (strong) | Near **zero** | Russell 2000 is explicitly small-cap (high SMB). It is a blend index — roughly equal value and growth — so HML exposure should be near zero. |\n",
    "| **2× S&P 500** | ≈ **2.0** | Near zero | Near zero | A 2× leveraged fund amplifies all returns by 2, so $\\beta_{\\text{MKT}} \\approx 2$. S&P 500 is diversified across value/growth and is large-cap, so SMB ≈ 0 and HML ≈ 0. Alpha may be slightly negative due to daily rebalancing costs and financing. |\n",
    "\n",
    "**Important caveat:** These are *predictions*. Actual estimates will differ because of tracking error, rebalancing effects, and the specific sample period. Running the regression in Exercise 2 or 5 lets us check these predictions empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511040a",
   "metadata": {},
   "source": [
    "### Exercise 2 — Comparing ETFs (Hands-On)\n",
    "\n",
    "**Problem:** Run the FF3 regression on IWN (Small Cap Value). Compare its $R^2$ and $\\beta_{\\text{SMB}}$ to SPY and VTV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB03 Exercise 2 — Compare SPY, IWN, VTV factor loadings\n",
    "# ============================================================================\n",
    "\n",
    "tickers = ['SPY', 'IWN', 'VTV']\n",
    "results_dict = {}\n",
    "\n",
    "for tic in tickers:\n",
    "    raw = yf.download(tic, start='2010-01-01', end='2023-12-31', progress=False)\n",
    "    monthly = get_close(raw, tic).resample('MS').first().pct_change().dropna()\n",
    "    monthly.name = tic\n",
    "    df = ff3_short[['Mkt_RF', 'SMB', 'HML', 'RF']].join(monthly, how='inner')\n",
    "    df['Excess'] = df[tic] - df['RF']\n",
    "    Y = df['Excess'].values\n",
    "    X = sm.add_constant(df[['Mkt_RF', 'SMB', 'HML']].values)\n",
    "    res = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n",
    "    results_dict[tic] = res\n",
    "\n",
    "# Pretty summary table\n",
    "labels = ['Alpha', 'Mkt-RF', 'SMB', 'HML']\n",
    "print(f\"{'':12s} {'SPY':>16s} {'IWN':>16s} {'VTV':>16s}\")\n",
    "print(\"=\" * 62)\n",
    "for i, lab in enumerate(labels):\n",
    "    row = f\"  {lab:8s}\"\n",
    "    for tic in tickers:\n",
    "        b = results_dict[tic].params[i]\n",
    "        se = results_dict[tic].bse[i]\n",
    "        star = '*' if results_dict[tic].pvalues[i] < 0.05 else ' '\n",
    "        row += f\"  {b:7.4f}{star}({se:.4f})\"\n",
    "    print(row)\n",
    "print(\"-\" * 62)\n",
    "for tic in tickers:\n",
    "    r2 = results_dict[tic].rsquared\n",
    "    print(f\"  R²  {tic:6s}  {r2:.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION\n",
    "  • IWN (Small Cap Value) should have the HIGHEST R² because it has\n",
    "    strong exposure to ALL three factors (Mkt, SMB, HML), so the\n",
    "    3-factor model captures more of its variation.\n",
    "  • SPY is dominated by Mkt-RF alone — SMB and HML add little.\n",
    "  • β_SMB ranking:  IWN >> SPY (near 0) ≈ VTV (near 0 or slightly negative).\n",
    "    IWN is explicitly small-cap, while SPY and VTV are large/mega-cap.\n",
    "  • β_HML ranking:  IWN > VTV > SPY (likely negative).\n",
    "    Both IWN and VTV are value-tilted, while SPY is blend/slight growth.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba95fba",
   "metadata": {},
   "source": [
    "### Exercise 3 — Sensitivity of Newey-West Standard Errors\n",
    "\n",
    "**Problem:** Re-fit the SPY regression with `maxlags=0`, default, and `maxlags=8`. How sensitive are conclusions to lag choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe991a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB03 Exercise 3 — Newey-West Lag Sensitivity\n",
    "# ============================================================================\n",
    "\n",
    "# Use the SPY data from Exercise 2\n",
    "spy_res = results_dict['SPY']\n",
    "spy_model = spy_res.model                     # underlying OLS model\n",
    "\n",
    "lag_configs = {\n",
    "    'Classic OLS':  spy_model.fit(),\n",
    "    'NW lag=0':     spy_model.fit(cov_type='HAC', cov_kwds={'maxlags': 0}),\n",
    "    'NW lag=3':     spy_model.fit(cov_type='HAC', cov_kwds={'maxlags': 3}),\n",
    "    'NW lag=6 (default)': spy_model.fit(cov_type='HAC', cov_kwds={'maxlags': 6}),\n",
    "    'NW lag=8':     spy_model.fit(cov_type='HAC', cov_kwds={'maxlags': 8}),\n",
    "    'NW lag=12':    spy_model.fit(cov_type='HAC', cov_kwds={'maxlags': 12}),\n",
    "}\n",
    "\n",
    "labels = ['Alpha', 'Mkt-RF', 'SMB', 'HML']\n",
    "\n",
    "print(\"STANDARD ERRORS FOR SPY 3-FACTOR REGRESSION ACROSS LAG CHOICES\")\n",
    "print(\"=\" * 80)\n",
    "header = f\"{'Coeff':>10s}\"\n",
    "for name in lag_configs:\n",
    "    header += f\"  {name:>14s}\"\n",
    "print(header)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, lab in enumerate(labels):\n",
    "    row = f\"{lab:>10s}\"\n",
    "    for name, res in lag_configs.items():\n",
    "        row += f\"  {res.bse[i]:14.6f}\"\n",
    "    print(row)\n",
    "\n",
    "print()\n",
    "print(\"P-VALUES FOR ALPHA ACROSS LAG CHOICES\")\n",
    "for name, res in lag_configs.items():\n",
    "    print(f\"  {name:>22s}:  p = {res.pvalues[0]:.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION\n",
    "  • NW lag=0 corrects for heteroscedasticity ONLY (equivalent to\n",
    "    White/HC0 robust SEs). These are usually very close to classic OLS\n",
    "    for monthly data, which often shows little heteroscedasticity.\n",
    "  • As lags increase, NW incorporates more autocovariance structure.\n",
    "    Standard errors typically INCREASE modestly because serial\n",
    "    correlation in the errors is accounted for.\n",
    "  • For SPY at monthly frequency, the differences are usually small,\n",
    "    because monthly factor-model residuals have weak autocorrelation.\n",
    "  • KEY LESSON: If conclusions flip based on reasonable lag choices\n",
    "    (e.g., 4 vs 8), the result is fragile. A robust finding should\n",
    "    survive a range of bandwidth choices. Andrews (1991) or Newey-West\n",
    "    (1994) automatic bandwidth selection provides a principled default.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47b6fb",
   "metadata": {},
   "source": [
    "### Exercise 4 — Economic vs. Statistical Significance (Discussion)\n",
    "\n",
    "**Problem:** $\\hat{\\alpha} = 0.0003$ (0.03 %/month), $p = 0.04$. Is this statistically significant? Economically significant? What happens with NW standard errors?\n",
    "\n",
    "---\n",
    "\n",
    "**Model Answer:**\n",
    "\n",
    "1. **Statistically significant?** Yes — the p-value (0.04) is below the conventional 5 % threshold.\n",
    "\n",
    "2. **Economically significant?** Almost certainly **no**.\n",
    "   - 0.03 % per month $\\approx$ 0.36 % per year.\n",
    "   - Typical ETF expense ratios are 0.03–0.20 %; bid-ask spreads and market impact add more. After transaction costs the apparent alpha evaporates.\n",
    "   - A rational investor would not restructure their portfolio to capture 36 basis points per year, especially when that estimate is noisy.\n",
    "\n",
    "3. **Effect of Newey-West SEs:**\n",
    "   - Newey-West SEs are generally **larger** than classic OLS SEs because they correct for positive residual autocorrelation and heteroscedasticity.\n",
    "   - A larger SE → a smaller $t$-statistic → a **larger** p-value. The result that was barely significant at 5 % with classic OLS could easily become insignificant (e.g., $p = 0.08$) once robust SEs are used.\n",
    "\n",
    "**Take-away:** Statistical significance ≠ Economic significance. A tiny alpha with a low p-value may be a real pricing anomaly, but it is not an investable opportunity once real-world frictions are included. Always convert monthly alphas into annualised terms and compare against realistic trading costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3c8f9",
   "metadata": {},
   "source": [
    "### Exercise 5 — Extending to Another Asset (Challenge)\n",
    "\n",
    "**Problem:** Download a stock of your choice, compute monthly excess returns, run the FF3 regression with Newey-West SEs, and interpret the results with diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab093de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB03 Exercise 5 — Full Worked Example: AAPL\n",
    "# ============================================================================\n",
    "\n",
    "# Step 1: Download AAPL monthly returns\n",
    "aapl_raw = yf.download('AAPL', start='2010-01-01', end='2023-12-31', progress=False)\n",
    "aapl_monthly = get_close(aapl_raw, 'AAPL').resample('MS').first().pct_change().dropna()\n",
    "aapl_monthly.name = 'AAPL'\n",
    "\n",
    "# Step 2: Align with FF3 factors and compute excess returns\n",
    "df_aapl = ff3_short[['Mkt_RF', 'SMB', 'HML', 'RF']].join(aapl_monthly, how='inner')\n",
    "df_aapl['Excess'] = df_aapl['AAPL'] - df_aapl['RF']\n",
    "print(f\"Sample: {df_aapl.index[0].strftime('%Y-%m')} to {df_aapl.index[-1].strftime('%Y-%m')}  \"\n",
    "      f\"({len(df_aapl)} months)\")\n",
    "\n",
    "# Step 3: Run regression with Newey-West SEs\n",
    "Y = df_aapl['Excess'].values\n",
    "X = sm.add_constant(df_aapl[['Mkt_RF', 'SMB', 'HML']].values)\n",
    "res_aapl = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n",
    "print(res_aapl.summary(xname=['Alpha', 'Mkt-RF', 'SMB', 'HML'],\n",
    "                        title='AAPL — Fama-French 3-Factor (NW SEs)'))\n",
    "\n",
    "# Step 4: Diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "\n",
    "# 4a. Residual time series\n",
    "resids = res_aapl.resid\n",
    "axes[0, 0].plot(df_aapl.index, resids * 100, linewidth=0.7, color='steelblue')\n",
    "axes[0, 0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0, 0].set_title('Residuals Over Time', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Residual (%)')\n",
    "\n",
    "# 4b. Histogram + normal overlay\n",
    "axes[0, 1].hist(resids * 100, bins=30, density=True, alpha=0.7,\n",
    "                color='steelblue', edgecolor='white')\n",
    "x_grid = np.linspace(resids.min()*100, resids.max()*100, 200)\n",
    "axes[0, 1].plot(x_grid, stats.norm.pdf(x_grid, resids.mean()*100, resids.std()*100),\n",
    "                'r-', linewidth=2, label='Normal')\n",
    "axes[0, 1].set_title('Residual Distribution', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 4c. QQ plot\n",
    "stats.probplot(resids, dist='norm', plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot', fontweight='bold')\n",
    "axes[1, 0].get_lines()[0].set_markersize(3)\n",
    "\n",
    "# 4d. ACF of residuals\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(resids, lags=15, ax=axes[1, 1], alpha=0.05, color='steelblue')\n",
    "axes[1, 1].set_title('Residual ACF', fontweight='bold')\n",
    "\n",
    "plt.suptitle('AAPL — Regression Diagnostics', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "alpha_ann = res_aapl.params[0] * 12 * 100\n",
    "print(f\"\\nINTERPRETATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Annualised alpha: {alpha_ann:.2f}%\")\n",
    "print(f\"  β_Mkt: {res_aapl.params[1]:.3f}  — AAPL is a high-beta stock\")\n",
    "print(f\"  β_SMB: {res_aapl.params[2]:.3f}  — Negative = large-cap behaviour\")\n",
    "print(f\"  β_HML: {res_aapl.params[3]:.3f}  — Negative = growth stock behaviour\")\n",
    "print(f\"  R²:    {res_aapl.rsquared:.3f}  — Low for a single stock (idiosyncratic risk)\")\n",
    "print(f\"\"\"\n",
    "  AAPL behaves as a large-cap growth stock with substantial \n",
    "  idiosyncratic risk. Its strong positive alpha over this period \n",
    "  reflects AAPL's extraordinary price appreciation that cannot be \n",
    "  explained by standard risk factors — consistent with the view \n",
    "  that single-stock alphas capture both true skill/innovation AND \n",
    "  idiosyncratic luck.\n",
    "  \n",
    "  Diagnostics: residuals likely show fat tails (QQ plot deviates \n",
    "  at extremes) — typical for individual stocks — confirming the \n",
    "  need for robust standard errors.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb5ceb",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 — Advanced Factor Models (Notebook 04)\n",
    "\n",
    "### Exercise 1 — 5-Factor Model on IWM\n",
    "\n",
    "**Problem:** Run the full 5-factor regression on IWM (Russell 2000 small-cap blend). Compare SMB loading to IWN, check HML significance, and compare 3-factor vs 5-factor alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB04 Exercise 1 — 5-Factor Regression on IWM\n",
    "# ============================================================================\n",
    "\n",
    "# Download IWM\n",
    "iwm_raw = yf.download('IWM', start='2000-01-01', end='2023-12-31', progress=False)\n",
    "iwm_monthly = get_close(iwm_raw, 'IWM').resample('MS').first().pct_change().dropna()\n",
    "iwm_monthly.name = 'IWM'\n",
    "\n",
    "# Also download IWN for comparison\n",
    "iwn_raw = yf.download('IWN', start='2000-01-01', end='2023-12-31', progress=False)\n",
    "iwn_monthly = get_close(iwn_raw, 'IWN').resample('MS').first().pct_change().dropna()\n",
    "iwn_monthly.name = 'IWN'\n",
    "\n",
    "# Merge with 5-factor data\n",
    "factor_cols_3 = ['Mkt_RF', 'SMB', 'HML', 'RF']\n",
    "factor_cols_5 = ['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']\n",
    "\n",
    "def run_3f_5f(monthly_ret, label):\n",
    "    \"\"\"Run both 3-factor and 5-factor regressions and print comparison.\"\"\"\n",
    "    df = factors[factor_cols_5].join(monthly_ret, how='inner').dropna()\n",
    "    df['Excess'] = df[monthly_ret.name] - df['RF']\n",
    "    Y = df['Excess'].values\n",
    "    \n",
    "    # 3-factor\n",
    "    X3 = sm.add_constant(df[['Mkt_RF', 'SMB', 'HML']].values)\n",
    "    res3 = sm.OLS(Y, X3).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n",
    "    \n",
    "    # 5-factor\n",
    "    X5 = sm.add_constant(df[['Mkt_RF', 'SMB', 'HML', 'RMW', 'CMA']].values)\n",
    "    res5 = sm.OLS(Y, X5).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {label}  —  {len(df)} months\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'':>14s}  {'3-Factor':>12s}  {'5-Factor':>12s}\")\n",
    "    print(f\"  {'─'*46}\")\n",
    "    \n",
    "    names_3 = ['Alpha', 'Mkt-RF', 'SMB', 'HML']\n",
    "    names_5 = ['Alpha', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "    for i, name in enumerate(names_5):\n",
    "        v3 = f\"{res3.params[i]:.4f}\" if i < 4 else \"—\"\n",
    "        v5 = f\"{res5.params[i]:.4f}\"\n",
    "        print(f\"  {name:>10s}    {v3:>12s}  {v5:>12s}\")\n",
    "    print(f\"  {'R²':>10s}    {res3.rsquared:>12.4f}  {res5.rsquared:>12.4f}\")\n",
    "    print(f\"  {'α p-val':>10s}    {res3.pvalues[0]:>12.4f}  {res5.pvalues[0]:>12.4f}\")\n",
    "    \n",
    "    return res3, res5\n",
    "\n",
    "iwm_3, iwm_5 = run_3f_5f(iwm_monthly, 'IWM (Russell 2000 — Small Cap Blend)')\n",
    "iwn_3, iwn_5 = run_3f_5f(iwn_monthly, 'IWN (Russell 2000 Value)')\n",
    "\n",
    "print(f\"\"\"\n",
    "INTERPRETATION\n",
    "{'='*60}\n",
    "1. SMB Comparison:\n",
    "   Both IWM and IWN have LARGE positive SMB loadings (both are\n",
    "   small-cap funds). IWN's SMB loading may be slightly higher\n",
    "   because small-cap value stocks tend to have even smaller\n",
    "   market caps on average.\n",
    "\n",
    "2. HML Significance:\n",
    "   - IWM (blend): HML loading near zero / insignificant\n",
    "     → blend of growth + value, so value factor washes out\n",
    "   - IWN (value): HML loading is LARGE and significant\n",
    "     → explicit value tilt captured by the model\n",
    "\n",
    "3. Alpha (3F vs 5F):\n",
    "   Adding RMW and CMA can change the alpha:\n",
    "   - If alpha shrinks in 5F → the fund's excess return was partly\n",
    "     explained by profitability/investment factors\n",
    "   - R² improvement shows whether extra factors add explanatory power\n",
    "\n",
    "4. RMW and CMA loadings:\n",
    "   Small-cap value stocks often load NEGATIVELY on RMW (lower\n",
    "   profitability) and positively on CMA (more conservative\n",
    "   investment) — consistent with the Fama-French interpretation\n",
    "   of the value premium.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1388502",
   "metadata": {},
   "source": [
    "### Exercise 2 — Rolling Sharpe Ratios for Combined Strategies\n",
    "\n",
    "**Problem:** Compute 36-month rolling Sharpe ratios for (a) HML alone, (b) Value + Profitability, (c) Value + Profitability + Momentum. Plot and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB04 Exercise 2 — Rolling Sharpe Ratios\n",
    "# ============================================================================\n",
    "\n",
    "# Build strategy returns (equal-weight combinations)\n",
    "strats = pd.DataFrame({\n",
    "    'HML only':        factors['HML'],\n",
    "    'Value + Prof':    (factors['HML'] + factors['RMW']) / 2,\n",
    "    'Val + Prof + Mom':(factors['HML'] + factors['RMW'] + factors['UMD']) / 3,\n",
    "})\n",
    "\n",
    "# Rolling 36-month annualised Sharpe ratio\n",
    "window = 36\n",
    "rolling_sharpe = (strats.rolling(window).mean() / strats.rolling(window).std()) * np.sqrt(12)\n",
    "rolling_sharpe = rolling_sharpe.dropna()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 5.5))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "for col, c in zip(rolling_sharpe.columns, colors):\n",
    "    ax.plot(rolling_sharpe.index, rolling_sharpe[col], label=col, linewidth=1.2, color=c)\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.fill_between(rolling_sharpe.index, 0, rolling_sharpe.min(axis=1),\n",
    "                where=rolling_sharpe.min(axis=1) < 0, alpha=0.1, color='red')\n",
    "ax.set_title('36-Month Rolling Sharpe Ratios: Single vs Combined Factor Strategies',\n",
    "             fontweight='bold', fontsize=13)\n",
    "ax.set_ylabel('Annualised Sharpe Ratio')\n",
    "ax.legend(loc='upper right', framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Strategy':>25s}  {'Mean SR':>8s}  {'Std SR':>8s}  {'% < 0':>8s}\")\n",
    "print(\"-\" * 60)\n",
    "for col in rolling_sharpe.columns:\n",
    "    sr = rolling_sharpe[col]\n",
    "    print(f\"{col:>25s}  {sr.mean():8.3f}  {sr.std():8.3f}  {(sr < 0).mean()*100:7.1f}%\")\n",
    "\n",
    "# Check if all three are simultaneously negative\n",
    "all_neg = (rolling_sharpe < 0).all(axis=1)\n",
    "if all_neg.any():\n",
    "    periods = rolling_sharpe.index[all_neg]\n",
    "    print(f\"\\nPeriods where ALL three strategies have negative rolling Sharpe:\")\n",
    "    print(f\"  {periods[0].strftime('%Y-%m')} to {periods[-1].strftime('%Y-%m')} \"\n",
    "          f\"({all_neg.sum()} months)\")\n",
    "else:\n",
    "    print(\"\\nNo period where ALL three strategies are simultaneously negative.\")\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION\n",
    "  • The combined strategy (green) has a HIGHER and MORE STABLE\n",
    "    rolling Sharpe than HML alone. Diversification across low-\n",
    "    correlated factors smooths returns.\n",
    "  • The biggest benefit shows during HML's worst periods (e.g.,\n",
    "    2018–2020 \"value winter\"): momentum and profitability partly\n",
    "    offset HML's losses.\n",
    "  • Adding momentum is especially valuable because UMD has near-\n",
    "    zero or negative correlation with HML, providing genuine\n",
    "    diversification.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3412ff",
   "metadata": {},
   "source": [
    "### Exercise 3 — Momentum Crash Anatomy\n",
    "\n",
    "**Problem:** Find the worst 3-month period for UMD. What was the market doing? How much would an equal-weight UMD+HML portfolio have reduced the drawdown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NB04 Exercise 3 — Momentum Crash Anatomy\n",
    "# ============================================================================\n",
    "\n",
    "# Part 1: Worst 3-month period for UMD\n",
    "rolling_3m_umd = factors['UMD'].rolling(3).sum()\n",
    "worst_end = rolling_3m_umd.idxmin()\n",
    "worst_start = worst_end - pd.DateOffset(months=2)\n",
    "\n",
    "print(\"WORST 3-MONTH PERIOD FOR MOMENTUM (UMD)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Period:  {worst_start.strftime('%Y-%m')} to {worst_end.strftime('%Y-%m')}\")\n",
    "print(f\"  3-month cumulative UMD return: {rolling_3m_umd.loc[worst_end]*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Show individual months\n",
    "crash_months = factors.loc[worst_start:worst_end, ['UMD', 'Mkt_RF', 'HML']]\n",
    "print(\"  Monthly detail:\")\n",
    "print(f\"  {'Month':>10s}  {'UMD':>8s}  {'Mkt-RF':>8s}  {'HML':>8s}\")\n",
    "for date, row in crash_months.iterrows():\n",
    "    print(f\"  {date.strftime('%Y-%m'):>10s}  {row['UMD']*100:8.2f}%  \"\n",
    "          f\"{row['Mkt_RF']*100:8.2f}%  {row['HML']*100:8.2f}%\")\n",
    "\n",
    "# Part 2: Market context\n",
    "mkt_3m = factors['Mkt_RF'].rolling(3).sum()\n",
    "print(f\"\\n  Market (Mkt-RF) over same 3 months: {mkt_3m.loc[worst_end]*100:+.2f}%\")\n",
    "print(\"\"\"\n",
    "  Daniel & Moskowitz (2016) narrative: Momentum crashes occur when\n",
    "  the market RECOVERS sharply after a downturn. Past losers (now\n",
    "  high-beta, beaten-down stocks) surge on the recovery, while past\n",
    "  winners (defensive/low-beta stocks) lag. The momentum portfolio\n",
    "  is short the rebounding losers and long the lagging winners,\n",
    "  creating catastrophic losses.\n",
    "\"\"\")\n",
    "\n",
    "# Part 3: Hedged portfolio\n",
    "hedged = (factors['UMD'] + factors['HML']) / 2  # equal-weight combo\n",
    "rolling_3m_hedged = hedged.rolling(3).sum()\n",
    "\n",
    "print(\"HEDGING EFFECT: UMD + HML (equal weight)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Worst 3-month UMD alone:   {rolling_3m_umd.loc[worst_end]*100:+.2f}%\")\n",
    "print(f\"  Hedged portfolio same dates: \"\n",
    "      f\"{rolling_3m_hedged.loc[worst_end]*100:+.2f}%\")\n",
    "print(f\"  Drawdown reduction:          \"\n",
    "      f\"{(1 - rolling_3m_hedged.loc[worst_end]/rolling_3m_umd.loc[worst_end])*100:.1f}%\")\n",
    "\n",
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Cumulative returns around the crash\n",
    "window_start = worst_start - pd.DateOffset(months=6)\n",
    "window_end   = worst_end   + pd.DateOffset(months=6)\n",
    "subset = factors.loc[window_start:window_end, ['UMD', 'HML', 'Mkt_RF']].copy()\n",
    "subset['UMD+HML'] = (subset['UMD'] + subset['HML']) / 2\n",
    "cum = (1 + subset[['UMD', 'HML', 'UMD+HML', 'Mkt_RF']]).cumprod()\n",
    "\n",
    "for col, c in zip(['UMD', 'UMD+HML', 'HML', 'Mkt_RF'],\n",
    "                   ['red', 'darkorange', 'steelblue', 'gray']):\n",
    "    ls = '--' if col == 'Mkt_RF' else '-'\n",
    "    axes[0].plot(cum.index, cum[col], label=col, linewidth=1.8, color=c, linestyle=ls)\n",
    "\n",
    "axes[0].axvspan(worst_start, worst_end, alpha=0.15, color='red', label='Crash window')\n",
    "axes[0].set_title('Cumulative Returns Around Momentum Crash', fontweight='bold')\n",
    "axes[0].set_ylabel('Growth of $1')\n",
    "axes[0].legend(loc='lower left', fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Full-sample max drawdowns\n",
    "for label, series in [('UMD', factors['UMD']),\n",
    "                       ('UMD+HML', hedged)]:\n",
    "    cum_full = (1 + series).cumprod()\n",
    "    running_max = cum_full.cummax()\n",
    "    dd = (cum_full - running_max) / running_max\n",
    "    axes[1].plot(dd.index, dd * 100, label=label, linewidth=1)\n",
    "\n",
    "axes[1].set_title('Maximum Drawdown Over Time', fontweight='bold')\n",
    "axes[1].set_ylabel('Drawdown (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "KEY INSIGHTS\n",
    "  • Momentum's worst crashes coincide with sharp market rebounds.\n",
    "  • HML often does well when momentum suffers (value stocks = past\n",
    "    losers that recover first), making it a natural hedge.\n",
    "  • The equal-weight UMD+HML combination substantially reduces\n",
    "    the worst drawdown, albeit at the cost of lower average return.\n",
    "  • This is a practical example of why multi-factor portfolios\n",
    "    dominate single-factor bets in risk-adjusted terms.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e048f",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Solutions Manual\n",
    "\n",
    "All 17 exercises from Notebooks 01–04 have been solved above. Key themes across the solutions:\n",
    "\n",
    "- **Robust inference matters.** Newey-West (HAC) standard errors are essential whenever residuals exhibit autocorrelation or heteroscedasticity — which is essentially always in financial data.\n",
    "- **Statistical ≠ Economic significance.** Always convert monthly estimates to annualised terms and compare against realistic transaction costs.\n",
    "- **Diagnostics before conclusions.** Check residual plots, ACF/PACF, ARCH tests, and stationarity before trusting regression output.\n",
    "- **Diversification across factors works.** Combining low-correlated factors (Value + Profitability + Momentum) materially improves risk-adjusted performance and reduces tail risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
